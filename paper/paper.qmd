---
title: "My title"
subtitle: "My subtitle if needed"
author: 
  - Andrew Goh
  - Yisu Hou
thanks: "Code and data are available at: [https://github.com/RohanAlexander/starter_folder](https://github.com/RohanAlexander/starter_folder)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
header-includes:
  - \usepackage{float}
  - \usepackage{placeins}
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(caret)
library(glmnet)
library(nnet)
library(lubridate)
library(pROC)
library(stringr)
library(gt)
```


# Introduction

Overview paragraph

note: For the literature review/why our thing matters section talk about how other predictions don't consider the biases in polls. Our thing is something new because we attempt to account for the bias in historical predictions for our 2024 prediction. (basically, for all models and predictions, I have an adjusted version that adjusts the support percentage using the difference between 2020 polls and real election outcomes, and a normal version that just uses the percentages from 2024 polls)

There's essentially 3 different levels of analysis: using multiple linear regression to look at the candidates' support over time on the national level, using logistic regression to predict the chance of winning on the national level, using state polls data to see who is ahead in electoral college votes.

Estimand paragraph

Results paragraph

Why it matters paragraph

Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....






# Data {#sec-data}

## Overview

We use the statistical programming language R [@R] for the graphing, analysis and presentation of the project as a whole. Caret [@caret], glmnet [@glmnet; @glmnet2; @glmnet3], nnet [@nnet], lubridate [@lubridate] and pROC [@pROC] were used directly in the analysis of the data. Tidyverse [@tidyverse], stringr [@stringr], dplyr [@dplyr], and styler [@styler] were used in the presentation and/or styling of the data, graphs, and paper as a whole.

In order to build a model that predicts the results of the 2024 election, we use survey/poll data on likely voters to inform our insights regarding likely election results, an approach widely accepted in political contexts [@politicalpolling]. We use a poll-of-polls dataset from 538 [@538data], consisting of aggregated polling data from FiveThirtyEight's 2024 presidential election polling database from various national and state-level polls from pollsters all around the country, recording information such as the date when the poll concluded, the entity conducting the poll, and the estimated support rate of the candidate as given by the poll. The data spans national and state-level polls, with particular focus on key battleground states that could determine the Electoral College outcome. This dataset was chosen over alternatives like RealClearPolitics because of 538's comprehensive methodological adjustments and transparent quality standards for included polls.

After cleaning the data to split results by candidate, removing polls with N/A values and ratings lower than 2.5, and further selecting polls polling likely voters as opposed to the whole population, we are left with 746 data points with the variables pollster, has_sponsor, numeric_grade, pollscore, transparency_score, sample_size, end_date, state, candidate_name, pct (support rate in percentage), and cycle (election cycle, i.e. 2020/2024).

## Measurement

The "from-voter-to-data" process for the 538 dataset involves three processes: measurement, where voter responses are collected and adjusted to best provide insight into a true support percentage population parameter, data collection, where data from polls is obtained and cleaned by 538 to only include polls adhering to their set of standards, and aggregation, where additional scores/variables are given to each poll to reflect their accuracy, transparency and bias levels with regards to polling scores [@5381].

The measurement process involves several sequential steps, beginning with initial data collection where pollsters conduct surveys using various modes such as phone, online, or mixed-mode approaches to gather voter preferences. Sample processing follows, involving likely voter screens to identify probable voters, demographic weighting to match population benchmarks, and various adjustments to ensure representativeness [@quinnipiac; @newyorktimes].

The data collection process involves obtaining data from various election polls from around the country, given that they meet specific methodological criteria. Each included poll must provide clear documentation of pollster identity, survey dates, and sampled population, maintain a minimum sample size of 100 respondents, and demonstrate transparent methodology including polling mode, sample source, and weighting procedures. Scientific sampling methods attempting to achieve representative samples are mandatory, as is the disclosure of poll sponsorship and funding sources. The dataset explicitly excludes non-scientific polls lacking representative sampling, MRP-smoothed data, recontact surveys, DIY polls from nonprofessional sources, polls with leading questions or hypothetical matchups, and subsamples from multi-state polls without geographic verification [@5382].

The aggregation phase, finally, involves rating polls by pollster polling quality (ranging from 0.5 to 3.0 stars), with polls with higher historical accuracy, lower consistent partisan biases, and higher transparency of methodology boasting higher ratings [@5383]. Transparency is also a rating and is done through a 10-question yes/no checklist on how much of the methodology is available to the public - note how this is completely separate from the quality of the methodology itself. Each question is graded on a 0/0.5/1 scale, for a combined transparency score that ranges from 0 to 10. Most polls in our dataset miss the mark on one or two criteron, netting a final transparency score of 9 - this is partially due to the data cleaning process removing many relatively worsely conducted polls.

Several important measurement limitations to FiveThirtyEight's methodology must be acknowledged. These include temporal gaps between polls creating discontinuous measurement of voter sentiment, response rates and participation biases potentially skewing samples, and varying geographic coverage with swing states being overrepresented. Additionally, third-party candidate treatment varies across polls, and is difficult to standardise (with respect to other data points) consistently.

## Outcome variable

The outcome variable is the level of support for the two leading US presidential candidates, Vice President Kamala Harris and Former President Donald Trump, recorded as a percentage of likely voters. For the national-level analysis, the support percentage represents the percentage of polled individuals who responded in favor of a specific candidate to a question that asked about their preferred presidential candidate in a national-level poll. For the regional datasets, the support percentage represents the percentage of likely voters who preferred a specific candidate in a poll that targets a specific state or congressional district of the United States. @fig-outcome shows the distribution of support percentages in the aggregated polling data from 2020 and 2024 presidential elections.

```{r}
#| echo: false
#| message: false
#| warning: false

trump_2024 <- read_csv(here::here("data/02-analysis_data/trump_2024_analysis_data.csv"))
harris_2024 <- read_csv(here::here("data/02-analysis_data/harris_2024_analysis_data.csv"))
trump_2024_lower <- 
  read_csv(here::here("data/02-analysis_data/trump_2024_analysis_data_lower.csv"))
harris_2024_lower <- 
  read_csv(here::here("data/02-analysis_data/harris_2024_analysis_data_lower.csv"))
trump_2020 <- read_csv(here::here("data/02-analysis_data/trump_2020_analysis_data.csv"))
biden_2020 <- read_csv(here::here("data/02-analysis_data/biden_2020_analysis_data.csv"))

# aggregated dataset for summary statistics
# datasets with lower numeric scores already includes polls with a score of 3.0
all_combined <- bind_rows(harris_2024_lower, trump_2024_lower,
    biden_2020, trump_2020)

```

```{r}
#| label: fig-outcome
#| fig-cap: Percentage Support in US Presidential Polls
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = pct)) +
  geom_histogram(
    binwidth = 1,                # Adjust bin width for granularity
    fill = "#2c7bb6",            # Aesthetic fill color (steel blue)
    color = "white",             # Border color for bins
    alpha = 0.8                   # Transparency of the fill
  ) +
  labs(
    x = "Percentage Support",
    y = "Number of Polls"
  ) +
  theme_minimal(base_size = 14) +   # Minimal theme with increased base font size
  theme(
    plot.title = element_text(
      size = 16, 
      face = "bold", 
      hjust = 0.5,                # Center the title
      margin = margin(b = 10)     # Add space below the title
    ),
    plot.subtitle = element_text(
      size = 12, 
      hjust = 0.5, 
      margin = margin(b = 20)     # Add space below the subtitle
    ),
    axis.title = element_text(face = "bold"), # Bold axis titles
    axis.text = element_text(color = "gray30"), # Dark gray axis text
    panel.grid.major = element_line(color = "gray80"), # Light gray major grid lines
    panel.grid.minor = element_blank()                 # Remove minor grid lines
  ) +
  # Add a vertical line for the mean percentage support
  geom_vline(
    aes(xintercept = mean(pct, na.rm = TRUE)),
    color = "#d73027",           # Aesthetic color (red)
    linetype = "dashed", 
    size = 1
  ) +
  # Annotate the mean value on the plot
  annotate(
    "text",
    x = mean(all_combined$pct, na.rm = TRUE) - 7, # Position text slightly to the right of the mean line
    y = Inf,                                     # Position text at the top of the plot
    label = paste("Mean:", round(mean(all_combined$pct, na.rm = TRUE), 1), "%"),
    vjust = 2,                                   # Vertical adjustment
    color = "#d73027",
    size = 5,
    fontface = "bold"
  )

```

\FloatBarrier

The aggregated polling data includes support percentages of different presidential candidates in polls with different FiveThirtyEight ratings [@5382] across two elections. Please view [UPDATE APPENDIX] for summary statistics of sub-datasets.

As displayed by @fig-outcome, the distribution of support percentages is close to a normal distribution, with most data points between 45 and 52 percent.

## Predictor variables

The predictor variables that we ended up using in our final logistic regression model were the poll end dates, transparency score, and sample sizes of the polls, as well as whether or not a poll was sponsored. The poll end dates were included to capture the temporal changes in candidates' support rate. Transparency score was selected as an identification of the polls' trustworthiness, which must be controlled for in the analysis. Other variables representing the poll quality in the raw dataset, such as the poll score and the FiveThirtyEight rating, were excluded from the models for two reasons. First, the documentation by FiveThirtyEight suggests that the numeric rating, poll score, and transparency score are highly related [@5382]. Second, the data cleaning process filtered data points with high FiveThirtyEight ratings and poll scores. The distributions of selected variables are as follows:

### End Date
```{r}
#| label: fig-dates-past
#| fig-cap: Distribution of Poll End Date, 2020 Cycle
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined %>% filter(cycle == 2020), aes(x = end_date)) +
  geom_histogram(binwidth = 7, fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "End Date",
    y = "Number of Polls"
  ) +
  scale_x_date(date_labels = "%b %d, %Y", date_breaks = "1 month") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
\vspace{1em}
\FloatBarrier

```{r}
#| label: fig-dates-current
#| fig-cap: Distribution of Poll End Date, 2024 Cycle
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined %>% filter(cycle == 2024), aes(x = end_date)) +
  geom_histogram(binwidth = 7, fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "End Date",
    y = "Number of Polls"
  ) +
  scale_x_date(date_labels = "%b %d, %Y", date_breaks = "1 month") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
\FloatBarrier
The end date indicates the date that a poll ends on. The dates are graphed separately between the 2020 and 2024 cycles for ease of viewing. Notice how there is a disproportionately large uptick in the number of polls that end on the end of a month.

### Sponsors

```{r}
#| label: fig-sponsored
#| fig-cap: Number of Polls with and without Sponsors
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = factor(has_sponsor, levels = c(0,1), labels = c("No Sponsor", "Has Sponsor")))) +
  geom_bar(fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "Has Sponsor",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold")
  )

```

Has_sponsor is a categorical variable that keeps track of whether the poll is sponsored by a third party other than the one conducting the poll itself. This is distributed roughly 60-40, with the larger portion of polls being independently funded (i.e. no sponsors).

\FloatBarrier

### Transparency Score

```{r}
#| label: fig-transparency
#| fig-cap: Distribution of Poll Transparency Scores
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = factor(transparency_score))) +
  geom_bar(fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "Transparency Score",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold")
  )

```

The transparency score is a metric given to a poll/pollster by 538 that measures the transparency of the methodology with which the poll is conducted. This is done through a 10-question yes/no checklist on how much of the methodology is available to the public - note how this is completely separate from the quality of the methodology itself. Each question is graded on a 0/0.5/1 scale, for a combined transparency score that ranges from 0 to 10 [@5383]. Most polls in our dataset miss the mark on one or two criteron, netting a final transparency score of 9 - this is partially due to the data cleaning process removing many relatively worsely conducted polls.

\FloatBarrier

### Sample Size

```{r}
#| label: fig-sample
#| fig-cap: Distribution of Poll Sample Sizes
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "Sample Size",
    y = "Number of Polls"
  ) +
  scale_x_continuous(labels = scales::comma) +
  coord_cartesian(xlim = c(0, 6000)) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold")
  )

```

Sample size, finally, is a measure of the number of participants in each poll. Around 1000 participants seems to overwhelmingly be the final survey size that pollsters decide on, with only single-digit numbers of polls (with rating >= 2.5) boasting more than four thousand respondents.

\FloatBarrier

# Model

The goal of our modelling strategy is twofold. Firstly, we seek to predict the winning candidate on election day by calculating the expected win rate of Vice President Kamala Harris and Former President Donald Trump in the seven swing states: Pennsylvania, Georgia, North Carolina, Michigan, Arizona, Wisconsin, and Nevada [@nyt2024polls]. Secondly, we hope to predict the candidate who has the popular vote on election day, controlling for the potential bias of national polls by accounting for the difference between polled popular vote and the actual popular vote on election day for the 2020 cycle.

To achieve the first goal, the polls dataset was divided by individual states, and only data points from the seven swing states were selected, forming seven smaller datasets. Then, for each state, the harris_win variable was constructed by comparing the support rate of the candidates in each poll. The variable was set to 1 if the support rate of Harris is greater than or equal to Trump's support rate and 0 when Trump's support rate is greater. A logistic model was constructed for each state, using harris_win as the outcome and previously stated predictors as inputs. [EXPLAIN THE PROBABILITY STEP HERE]

For the popular vote, we utilized the data from 2020 to calculate each pollster's bias by identifying the average difference between the popular vote on election day and their polled national support rates of Trump and President Joe Biden. Next, we adjusted the polled national support rates of pollsters in the 2024 cycle by the calculated gap in 2020. By making this adjustment, we are assuming that the pollsters systematically bias 2024 candidates equally compared to 2020 candidates. We acknowledge that the assumption is most likely false. Ideally, incorporating poll data from earlier election cycles may produce a more accurate indicator of how pollsters systematically bias candidates from the two dominant political parties. However, comprehensive poll data before 2020 is unavailable. Since popular vote is unrelated to the winner of the election, we are using this analysis as an exploration of methods that incorporate historical data without emphasizing predictiveness. Using both adjusted and unadjusted national support levels, multiple linear regressions were created with support levels as the outcome and stated predictors as inputs to predict candidates' support rate over time. Adjusted and unadjusted logistic regressions were made using harris_win as the outcome and the same predictors to predict the candidate with the popular vote on election day. The attempt to control for historical bias was not applied to the swing states due to a lack of data.

## Model set-up

### Logistic models
Define $y_i$ as the binary outcome variable indicating whether Harris wins (1) or not (0) for the $i^{th}$ observation. $x_{1i}$ is the end_date for the $i^{th}$ observation. $x_{2i}$ is the binary value of has_sponsor for the $i^{th}$ observation. $x_{3i}$ is the transparency_score for the $i^{th}$ observation. $x_{4i}$ is the sample_size for the $i^{th}$ observation.

The logistic regression model can be expressed as follows:

\begin{align} y_i &\sim \text{Bernoulli}(p_i) 
\\ \text{logit}(p_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} \end{align}

Line (1) specifies that the outcome variable $y_i$ follows a Bernoulli distribution with probability $p_i$ of success (i.e., Harris winning). Line (2) links the linear combination of predictors to the probability $p_i$, where $\beta_0$ is the intercept term, and $\beta_1, \beta_2, \beta_3, \beta_4$ are coefficients corresponding to each predictor variable above. 

This logistic regression model was applied independently to the data on each swing state and the adjusted and unadjusted popular vote polls. 

### Multiple linear regression models
Define $y_i$ as the percentage national support for Harris or Trump for the $i^{th}$ observation. $x_{1i}$ is the end_date for the $i^{th}$ observation. $x_{2i}$ is the binary value of has_sponsor for the $i^{th}$ observation. $x_{3i}$ is the transparency_score for the $i^{th}$ observation. $x_{4i}$ is the sample_size for the $i^{th}$ observation.

The multiple linear regression model can be expressed as follows:

\begin{align} y_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} \end{align}

Where $\beta_0$ is the intercept term, and $\beta_1, \beta_2, \beta_3, \beta_4$ are coefficients corresponding to each predictor variable.

Separate multiple linear regression models were applied to the polled national support rate of Harris and Trump, with and without adjustments from historical data. 

### Model justification

We decided to use a logistic model to predict the winner on election day because it directly outputs candidates' chance of winning each swing state, enabling us to use a probabilistic function to identify their overall chance of winning. Comparatively, a linear regression model merely predicts the level of support for candidates, which fails to identify a winner. 

```{r}
#| echo: false
#| message: false
#| warning: false

# Regional polls analysis
trump_2024_regional <- trump_2024_lower %>% filter(state != "National")
harris_2024_regional <- harris_2024_lower %>% filter(state != "National")


trump_Pennsylvania <- trump_2024_regional %>% filter(state == "Pennsylvania")
trump_Georgia <- trump_2024_regional %>% filter(state == "Georgia")
trump_North_Carolina <- trump_2024_regional %>% filter(state == "North Carolina")
trump_Michigan <- trump_2024_regional %>% filter(state == "Michigan")
trump_Arizona <- trump_2024_regional %>% filter(state == "Arizona")
trump_Wisconsin <- trump_2024_regional %>% filter(state == "Wisconsin")
trump_Nevada <- trump_2024_regional %>% filter(state == "Nevada")

harris_Pennsylvania <- harris_2024_regional %>% filter(state == "Pennsylvania")
harris_Georgia <- harris_2024_regional %>% filter(state == "Georgia")
harris_North_Carolina <- harris_2024_regional %>% filter(state == "North Carolina")
harris_Michigan <- harris_2024_regional %>% filter(state == "Michigan")
harris_Arizona <- harris_2024_regional %>% filter(state == "Arizona")
harris_Wisconsin <- harris_2024_regional %>% filter(state == "Wisconsin")
harris_Nevada <- harris_2024_regional %>% filter(state == "Nevada")

combined_Pennsylvania <- full_join(
  trump_Pennsylvania,
  harris_Pennsylvania,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Georgia <- full_join(
  trump_Georgia,
  harris_Georgia,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_North_Carolina <- full_join(
  trump_North_Carolina,
  harris_North_Carolina,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Michigan <- full_join(
  trump_Michigan,
  harris_Michigan,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Arizona <- full_join(
  trump_Arizona,
  harris_Arizona,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Wisconsin <- full_join(
  trump_Wisconsin,
  harris_Wisconsin,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Nevada <- full_join(
  trump_Nevada,
  harris_Nevada,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

# mutate harris_win variable by comparing polled support rates
# preparing data for logistic regression
combined_Pennsylvania <- combined_Pennsylvania %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Georgia <- combined_Georgia %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_North_Carolina <- combined_North_Carolina %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Michigan <- combined_Michigan %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Arizona <- combined_Arizona %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Wisconsin <- combined_Wisconsin %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Nevada <- combined_Nevada %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

# Create 7 different logistic regressions to predict Harris's win rate
# in each state
logistic_Pennsylvania <- readRDS(here::here("models/Logistic_model_Pennsylvania.rds"))

logistic_Georgia <- readRDS(here::here("models/Logistic_model_Georgia.rds"))

logistic_North_Carolina <- readRDS(here::here("models/Logistic_model_North_Carolina.rds"))

logistic_Michigan <- readRDS(here::here("models/Logistic_model_Michigan.rds"))

logistic_Arizona <- readRDS(here::here("models/Logistic_model_Arizona.rds"))

logistic_Wisconsin <- readRDS(here::here("models/Logistic_model_Wisconsin.rds"))

logistic_Nevada <- readRDS(here::here("models/Logistic_model_Nevada.rds"))

# test logistic models for state specific analysis
# mutate fitted values

combined_Pennsylvania <- combined_Pennsylvania %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Pennsylvania, type = "response"), 2)
  )

roc_obj_Pennsylvania <- roc(combined_Pennsylvania$harris_win,
                  combined_Pennsylvania$harris_win_prob)

auc_value_Pennsylvania <- auc(roc_obj_Pennsylvania)


combined_Georgia <- combined_Georgia %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Georgia, type = "response"), 2)
  )

roc_obj_Georgia <- roc(combined_Georgia$harris_win,
                            combined_Georgia$harris_win_prob)

auc_value_Georgia <- auc(roc_obj_Georgia)


combined_North_Carolina <- combined_North_Carolina %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_North_Carolina, type = "response"), 2)
  )

roc_obj_North_Carolina <- roc(combined_North_Carolina$harris_win,
                            combined_North_Carolina$harris_win_prob)

auc_value_North_Carolina <- auc(roc_obj_North_Carolina)


combined_Michigan <- combined_Michigan %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Michigan, type = "response"), 2)
  )

roc_obj_Michigan <- roc(combined_Michigan$harris_win,
                            combined_Michigan$harris_win_prob)

auc_value_Michigan <- auc(roc_obj_Michigan)

combined_Arizona <- combined_Arizona %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Arizona, type = "response"), 2)
  )

roc_obj_Arizona <- roc(combined_Arizona$harris_win,
                            combined_Arizona$harris_win_prob)

auc_value_Arizona <- auc(roc_obj_Arizona)


combined_Wisconsin <- combined_Wisconsin %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Wisconsin, type = "response"), 2)
  )

roc_obj_Wisconsin <- roc(combined_Wisconsin$harris_win,
                            combined_Wisconsin$harris_win_prob)

auc_value_Wisconsin <- auc(roc_obj_Wisconsin)


combined_Nevada <- combined_Nevada %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Nevada, type = "response"), 2)
  )

roc_obj_Nevada <- roc(combined_Nevada$harris_win,
                            combined_Nevada$harris_win_prob)

auc_value_Nevada <- auc(roc_obj_Nevada)

```

We identified the AUC values and ROC plots for each of the seven swing-state logistic regressions. The ROC plots are presented in the Appendix, from @fig-pen to @fig-nev. The AUC values are: 0.90 for Pennsylvania, 0.88 for Georgia, 0.74 for North Carolina, 0.93 for Michigan, 1.00 for Arizona, 0.86 for Wisconsin, and 0.65 for Nevada. All states other than Nevada have acceptable or excellent AUC values, indicating that the respective logistic models are reliable when distinguishing cases where Harris wins. The low AUC value for Nevada is potentially caused by the limited sample size; there are only 9 polls conducted in Nevada.

Moreover, the sample size restrictions were the reason why we did not use training and testing data to validate our logistic models. The state with the greatest number of polls, Pennsylvania, only records 26 polls. Utilizing training and testing datasets implies that the datasets may include only a couple of data points in some states, which is insufficient to construct or test the model. On top of that, according to [@trainingerror], achieving low training error is sufficient to ensure the predictability of a model to some extent. [ADD MORE IF NECESSARY]

```{r}
#| echo: false
#| message: false
#| warning: false

# find biases of pollsters from 2020 national polls data
# filter out national polls from the datasets
trump_2024_national <- trump_2024 %>% filter(state == "National")
harris_2024_national <- harris_2024 %>% filter(state == "National")
trump_2020_national <- trump_2020 %>% filter(state == "National")
biden_2020_national <- biden_2020 %>% filter(state == "National")

# Define actual results
actual_results_2020 <- list(Trump_pct = 47, Biden_pct = 51)

# Calculate bias per pollster
rep_pollster_bias <- trump_2020_national %>%
  group_by(pollster) %>%
  summarize(
    Trump_bias = mean(pct - actual_results_2020$Trump_pct, na.rm = TRUE)
  )

dem_pollster_bias <- biden_2020_national %>%
  group_by(pollster) %>%
  summarize(
    Biden_bias = mean(pct - actual_results_2020$Biden_pct, na.rm = TRUE)
  )

# adjust 2024 national polls percentages by the average pollster biases
# also mutate has_sponsor variable into dummy variable and remove NA's
trump_2024_national_adj <- trump_2024_national %>%
  left_join(rep_pollster_bias, by = "pollster") %>%
  mutate(
    pct_adj = pct - ifelse(!is.na(Trump_bias), Trump_bias, 0)
  ) %>%
  select(-Trump_bias)

# Adjust Harris 2024 National Polls
harris_2024_national_adj <- harris_2024_national %>%
  left_join(dem_pollster_bias, by = "pollster") %>%
  mutate(
    pct_adj = pct - ifelse(!is.na(Biden_bias), Biden_bias, 0)
  ) %>%
  select(-Biden_bias)


# model adjusted support percentage by time and control variables
# first split training and testing dataset using an 80/20 split
set.seed(11451)
trainIndex_trump_adj <- createDataPartition(trump_2024_national_adj$pct_adj, 
                                        p = 0.8, list = FALSE)
data_train_trump_adj <- trump_2024_national_adj[trainIndex_trump_adj, ]
data_test_trump_adj  <- trump_2024_national_adj[-trainIndex_trump_adj, ]

trainIndex_harris_adj <- createDataPartition(harris_2024_national_adj$pct_adj, 
                                         p = 0.8, list = FALSE)
data_train_harris_adj <- harris_2024_national_adj[trainIndex_harris_adj, ]
data_test_harris_adj  <- harris_2024_national_adj[-trainIndex_harris_adj, ]

model_date_harris_adj <- lm(pct_adj ~ end_date + has_sponsor +
                          transparency_score + sample_size, data = data_train_harris_adj)
model_date_trump_adj <- lm(pct_adj ~ end_date + has_sponsor +
                         transparency_score + sample_size, data = data_train_trump_adj)

# repeat for unadjusted support levels
trainIndex_trump <- createDataPartition(trump_2024_national$pct, 
                                        p = 0.8, list = FALSE)
data_train_trump <- trump_2024_national[trainIndex_trump, ]
data_test_trump  <- trump_2024_national[-trainIndex_trump, ]

trainIndex_harris <- createDataPartition(harris_2024_national$pct, 
                                         p = 0.8, list = FALSE)
data_train_harris <- harris_2024_national[trainIndex_harris, ]
data_test_harris  <- harris_2024_national[-trainIndex_harris, ]

model_date_harris <- lm(pct ~ end_date + has_sponsor +
                          transparency_score + sample_size, data = data_train_harris)
model_date_trump <- lm(pct ~ end_date + has_sponsor +
                         transparency_score + sample_size, data = data_train_trump)


# Functions to evaluate model using testing dataset
evaluate_model_adj <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data)
  actuals <- test_data$pct_adj
  rmse <- sqrt(mean((predictions - actuals)^2, na.rm = TRUE))
  r_squared <- summary(model)$r.squared
  return(list(RMSE = rmse, R_squared = r_squared))
}

evaluate_model <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data)
  actuals <- test_data$pct
  rmse <- sqrt(mean((predictions - actuals)^2, na.rm = TRUE))
  r_squared <- summary(model)$r.squared
  return(list(RMSE = rmse, R_squared = r_squared))
}

# Evaluate Harris model with adjusted support percentages
harris_evaluation_adj <- evaluate_model_adj(model_date_harris_adj, data_test_harris_adj)


# Evaluate Trump model with adjusted support percentages
trump_evaluation_adj <- evaluate_model_adj(model_date_trump_adj, data_test_trump_adj)


# Evaluate Harris model with unadjusted support percentages
harris_evaluation <- evaluate_model(model_date_harris, data_test_harris)


# Evaluate Trump model with unadjusted support percentages
trump_evaluation <- evaluate_model(model_date_trump, data_test_trump)


# Augment data with model predictions
model_harris_adj <- readRDS(here::here("models/MLR_adjusted_harris.rds"))
model_trump_adj <- readRDS(here::here("models/MLR_adjusted_trump.rds"))

harris_2024_national_adj <- harris_2024_national_adj %>% mutate(
  fitted_date = predict(model_harris_adj)
)

trump_2024_national_adj <- trump_2024_national_adj %>% mutate(
  fitted_date = predict(model_trump_adj)
)

# Unadjusted datasets
model_harris <- readRDS(here::here("models/MLR_harris.rds"))
model_trump <- readRDS(here::here("models/MLR_trump.rds"))

harris_2024_national <- harris_2024_national %>% mutate(
  fitted_date = predict(model_harris)
)

trump_2024_national <- trump_2024_national %>% mutate(
  fitted_date = predict(model_trump)
)

# evaluate models without using training and testing data
harris_eval_adj <- evaluate_model_adj(model_harris_adj, harris_2024_national_adj)


harris_eval <- evaluate_model(model_harris, harris_2024_national)


trump_eval_adj <- evaluate_model_adj(model_trump_adj, trump_2024_national_adj)


trump_eval <- evaluate_model(model_trump, trump_2024_national)



#### Logistic regression for national support ####

combined_national <- full_join(
  trump_2024_national,
  harris_2024_national,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_national <- combined_national %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

# repeat for adjusted support levels
combined_national_adj <- full_join(
  trump_2024_national_adj,
  harris_2024_national_adj,
  by = c("pollster", "has_sponsor", "pollscore",
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_national_adj <- combined_national_adj %>% mutate(
  harris_win = case_when(
    pct_adj.y >= pct_adj.x ~ 1,
    pct_adj.y < pct_adj.x ~ 0
  )
)


# logistic model to predict the popular vote
model_logistic <- readRDS(here::here("models/Logistic_model.rds"))

model_logistic_adj <- readRDS(here::here("models/Logistic_model_adjusted.rds"))


# include predicted outcomes to the dataframes
combined_national <- combined_national %>%
  mutate(
    harris_win_prob = round(100*predict(model_logistic, type = "response"), 2)
  )

combined_national_adj <- combined_national_adj %>%
  mutate(
    harris_win_prob = round(100*predict(model_logistic_adj, 
                                        type = "response"), 2)
  )


#### Evaluate the Model ####

# ROC Curve and AUC
roc_obj <- roc(combined_national$harris_win,
               combined_national$harris_win_prob)
auc_value <- auc(roc_obj)

roc_obj_adj <- roc(combined_national_adj$harris_win,
               combined_national_adj$harris_win_prob)
auc_value_adj <- auc(roc_obj_adj)
```
For the analysis of the national popular vote, the logistic model of adjusted polls possess an AUC value of 0.98, and the logistic model of unadjusted polls have an AUC of 0.94, indicating high reliability of categorization. Their respective ROC plots are presented as @fig-log and @fig-logadj. 

RMSE values and R squared values were calculated for the multiple linear regression models predicting popular vote. The RMSE values for all four models, adjusted and unadjusted support levels of the two candidates, are greater than 1, indicating that the fitted values and the support levels of actual polls are more than 1 percent apart on average, which is significant when analyzing popular vote. Moreover, all R squared values other than the model for the adjusted support rate of Trump, are smaller than 0.3, indicating the inability of the models to account for the variance of national support rates in the data. The model for the adjusted support rate of Trump has a R squared value of 0.69, indicating a relatively high proportion of variance in polled support rates being explained by the multiple linear regression model. This was expected, as the predictors such as a poll's end date and transparency does not necessarily correlate with the outcome of the polls. More predictive variables used by FiveThirtyEight [@5383] for their predictions are unavailable in the raw dataset, which is a key limitation of the data.


# Results

Our results are summarized in @tbl-modelresults.

```{r}
# plot model predictions
# combine the datasets to create a single visual
combined_data_adj <- bind_rows(harris_2024_national_adj, trump_2024_national_adj)
combined_data <- bind_rows(harris_2024_national, trump_2024_national)

# create the plot that contains scattered points of adjusted percentages
# and lines for fitted values
ggplot(combined_data_adj, aes(x = end_date, y = pct_adj, color = candidate_name)) +
  # Scatter points for actual adjusted percentages
  geom_point(alpha = 0.6) +
  # Regression lines
  geom_line(aes(y = fitted_date), size = 0.5) +
  xlim(as.Date("2024-07-19"), as.Date("2024-10-25")) +
  # Assign specific colors
  scale_color_manual(values = 
                       c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  # Customize the plot appearance
  theme_classic() +
  labs(
    y = "Adjusted Percent Support",
    x = "End Date",
    title = "Multiple Linear Models for Harris and Trump (2024)",
    color = "Candidate"
  ) +
  theme(
    text = element_text(size = 12),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )

# Unadjusted support percentages
ggplot(combined_data, aes(x = end_date, y = pct, color = candidate_name)) +
  # Scatter points for actual adjusted percentages
  geom_point(alpha = 0.6) +
  # Regression lines
  geom_line(aes(y = fitted_date), size = 0.5) +
  xlim(as.Date("2024-07-19"), as.Date("2024-10-25")) +
  # Assign specific colors
  scale_color_manual(values = 
                       c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  # Customize the plot appearance
  theme_classic() +
  labs(
    y = "Adjusted Percent Support",
    x = "End Date",
    title = "Multiple Linear Models for Harris and Trump (2024)",
    color = "Candidate"
  ) +
  theme(
    text = element_text(size = 12),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )


```

```{r}
summary(model_logistic)
summary(model_logistic_adj)
```




# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.

\newpage

\appendix

# Appendix {-}
## Pollster methodology overview and evaluation
The Quinnipiac University Poll conducts independent polling in swing states. To analyze their methodology, we look specifically into their October 2024 Pennsylvania polls [@quinnipiac].

Their target population is likely voters aged 18 and older in Pennsylvania [@quinnipiacmethodology]. To reach this population, they use likely voters aged 18 and older with phone numbers (both landline and cell) as their sampling frame, i.e. the frame of possible subjects that they sample observations from. Quinnipiac University employed Random Digit Dialing (RDD) to generate their sample of 2,186 respondents. This dual-frame approach reflects modern communication patterns, with 1,644 cell phone and 542 landline completions. However, using phones as a sampling frame means they cannot reach voters without phone access, introducing potential coverage bias.

Their sampling approach uses stratification by Census division according to area code, meaning that they divide Pennsylvania into geographic regions before using RDD to sample within each region. This strategy ensures even geographic representation but adds complexity to the sampling process [@pollofpolls]. For each selected number, they attempt contact at least three times before marking it as non-responsive. For landline calls, they ask to speak with the household member who has the next birthday, a simple but effective randomization technique. Afterwards, a series of screening questions confirm that the subject is indeed a likely voter, after which the subject's responses are then formally taken as part of the sample.

After collecting responses, Quinnipiac adjusts their data through post-stratification weighting. In this weighting, they compare their sample's demographic composition to known population benchmarks from the Census (like age, gender, education, and race distributions in Pennsylvania) and adjust the weight given to each response to match these benchmarks. For example, if their sample has too few young voters compared to Census data, responses from young voters would be weighted more heavily. While this helps correct for sampling imbalances, it can increase the variance in their estimates if the weights vary substantially [@poststratification].

The survey administration addresses measurement issues through its design. Live interviewers conduct all interviews, enabling question clarification and generating higher response rates compared to automated systems. However, live interviewers may introduce social desirability bias, where respondents might modify their answers to appear more socially acceptable. This becomes particularly relevant in political polling, where respondents might hesitate to express unpopular political views [@politicalpolling].

Several types of bias affect the poll's results. Self-selection bias occurs because certain types of people (typically those more politically engaged or with stronger views) are more likely to agree to participate in the survey. Non-response bias arises when people who respond differ systematically from those who don't - for instance, busier people might be less likely to answer calls, potentially underrepresenting certain occupational groups [@nonresponse]. Coverage bias means some groups (like those without phones) have no chance of being included in the sample.

Ethics-wise, the survey does a good job of informing individuals of the details of the survey, such as its purpose and how participants' data is to be used. Personally-identifiable data (i.e. name, phone number, etc.) is not collected during the survey other than to ensure that repeat numbers are not drawn, and the usage of live interviewers to conduct the survey ensures that concerns about the ethics of the survey can be voiced and answered on-the-spot. Combined with the lack of financial incentive for participants, which suggests that all participants participate of their complete free will and intention, this is an ethically sound methodology for conducting a political survey.

Finally, the poll also faces common challenges in political polls such as this one - the five-day field period (October 24-28) may miss opinion changes close to election day, and while weighting adjustments help correct for demographic imbalances, they may increase variance in the estimates if some groups need to be weighted heavily to match population benchmarks, for example.

Overall, Quinnipiac's methodology represents a balanced approach to managing practical constraints and statistical rigor in modern political polling - while some common biases are still likely to skew the poll results off the true support levels for Harris, for example, the poll uses methods such as post-stratification weighting to tradeoff biases at the cost of model variance [@weighting]. Using live interviewers unavoidably introduces social desirability bias, however, and significantly increases poll costs per quota. Modifying the methodology to remove this aspect of the survey would potentially reduce bias and allow for larger samples to be taken, in turn opening up possibilities for cross-validation and the such, which then reduces the effect of increased model variance on the final results.

## Idealised methodology
With a $100,000 budget, our approach focuses on producing accurate state-level estimates in key battleground states, which would then inform our national forecast. We prioritize Pennsylvania, Michigan, Wisconsin, Georgia, Arizona, and Nevada, allocating resources proportionally based on each state's electoral importance and expected margin of victory.

Our sampling strategy employs both probability and non-probability methods. Probability sampling (60% of budget, i.e. $60k) means every member of our target population has a known, non-zero chance of being selected - this allows us to calculate proper margins of error and make statistical inferences about the population. For this, we use dual-frame random digit dialing (RDD) for phone surveys and address-based sampling (ABS) for mail-to-web recruitment. RDD involves generating random phone numbers within active area codes, while ABS uses the U.S. Postal Service's delivery database as a sampling frame, both proven methods of sampling [@cummings]. The ABS approach helps reach households without reliable phone access. We stratify our sample by geography, demographics, and previous voting patterns to ensure representation across key subgroups - meaning we divide the population into these subgroups and sample from each independently.

For non-probability sampling (40% of budget, i.e. $40k), where respondents' selection probabilities are unknown and not everyone has a chance of being selected, we recruit through multiple online panel vendors and use targeted social media advertising to reach traditionally underrepresented groups. While this approach introduces potential selection bias because participants self-select into the sample, it helps reach younger voters who are less responsive to traditional survey methods [@nonprobabilitysampling]. We implement quota sampling within these non-probability samples to match key demographic targets - for example, stopping collection from certain demographic groups once their quota is filled.

Respondent recruitment uses multiple contact methods - mail, email, text, and phone - with attempts made at different times and days to maximize response rates. We offer a $10 gift card incentive for completed surveys and provide both English and Spanish language options. This mixed-mode contact strategy helps reduce non-response bias by providing multiple ways to participate.

Data validation is crucial for maintaining quality. We cross-reference responses with voter files where available - meaning we check if respondents' self-reported registration status matches official records. We screen for duplicate responses using IP addresses and phone numbers, and implement attention checks within the survey (questions with known correct answers to ensure respondents are reading carefully). Speed checks identify rushed responses that might indicate low-quality data by flagging completions that fall below a minimum reasonable completion time, while consistency checks across related questions help identify potentially fraudulent responses by looking for logical contradictions in answers.

Our weighting approach uses post-stratification to known population benchmarks - this means we adjust the weight given to each response so that our sample matches known population characteristics. For example, if our sample has 30% college graduates but the population has 40%, we would give more weight to responses from college graduates. We include demographics (age, race, education, gender), geographic location, past voting behavior, and party registration in our weighting scheme. We produce daily estimates using a 7-day rolling average, which helps smooth out daily fluctuations while remaining responsive to real changes in voter preferences [@weighting2].

Finally, how ethics are handled is a crucial part of any survey methodology. In this idealised methodology, consent will be asked for at the beginning of the survey, full disclosure of how information is used will be given beforehand and no self-identifiable information will be recorded (so no names, phone numbers, etc.). The $10 incentive is enough to hopefully make it wortile for participants' time, but also not ideally not significant enough of an incentive to make individuals suppress otherwise deal-breaking concerns with the survey purely for the sake of the incentive.

The survey instrument itself focuses on six key areas: screening questions to identify likely voters, voting intentions (including direct questions about Trump vs. Harris preferences), political preferences, demographics, issue priorities, and media consumption patterns. We've implemented this survey design in Google Forms, which can be found here: https://forms.gle/pk7vDiMHwEGLMK849

This methodology balances statistical rigor with practical constraints, while acknowledging and attempting to address the key challenges in modern political polling: declining response rates, coverage bias, and the increasing difficulty of reaching a representative sample of likely voters.

## Idealised survey
The survey, made using Google Forms, is linked here: https://forms.gle/pk7vDiMHwEGLMK849
Note that the questions are identical for both the phone and online surveys bar q6.
A copy of the survey that is identical to the one implemented in the Google Forms above is presented below:
Thank you for participating in this survey about the 2024 U.S. Presidential Election. This survey is part of a research project at the University of Toronto studying voting intentions and political attitudes.

Estimated completion time: 8-10 minutes

Your responses will be kept confidential and used only for research purposes. Email information, and any other information that may personally identify you, is not gathered. You may skip any questions you prefer not to answer, though complete responses are most helpful for our research.

For questions or concerns about this survey, please contact: andrew.goh@mail.utoronto.ca

SCREENING SECTION:
Q1. Are you 18 years of age or older?

Yes
No [END SURVEY]

Q2. Are you a U.S. citizen?

Yes
No [END SURVEY]

Q3. Are you registered to vote at your current address?

Yes
No
Not sure
[If No or Not sure: Do you plan to register before the November 2024 election?]

VOTING INTENTION:
Q4. How likely are you to vote in the 2024 presidential election?

Definitely will vote
Probably will vote
Might or might not vote
Probably will not vote
Definitely will not vote

Q5. If the 2024 presidential election were held today, and the candidates were Kamala Harris (Democrat) and Donald Trump (Republican), who would you vote for?

Kamala Harris
Donald Trump
Another candidate (please specify)
Would not vote
Not sure

ATTENTION CHECK:
Q6. To ensure you're reading carefully, please select "Somewhat disagree" for this question: "I enjoy following political news."

Strongly agree
Somewhat agree
Somewhat disagree
Strongly disagree
No opinion

POLITICAL PREFERENCES:
Q7. Generally speaking, you consider yourself a:

Democrat
Republican
Independent
Something else (please specify)

Q8. How would you rate the current state of the U.S. economy?

Very poor
Poor
Fair
Good
Excellent

ISSUE PRIORITIES:
Q9. Which ONE of the following issues is most important to you when deciding how to vote?

Economy and jobs
Immigration
Healthcare
Climate change
Crime and public safety
Education
National security
Abortion rights
Gun policy
Something else (please specify)

For each of the following issues, please indicate whether you think Kamala Harris or Donald Trump would do a better job handling it:

Q10. Economy and jobs:

Kamala Harris would do better
Donald Trump would do better
No difference
Not sure

Q11. Human rights and freedom of speech:
[Same options]
Q12. Abortion:
[Same options]
Q13. Healthcare:
[Same options]
Q14. Immigration:
[Same options]
Q15. National security:
[Same options]

MEDIA CONSUMPTION:
Q16. Where do you most often get your news about politics? (Select all that apply)

Network TV news (ABC, CBS, NBC)
Cable TV news (CNN, Fox News, MSNBC)
Local TV news
Radio
Print newspapers
News websites
Social media
Friends and family
Other (please specify)

Q17. How many hours per day do you typically spend following news about politics?

Less than 1 hour
1-2 hours
2-4 hours
More than 4 hours

DEMOGRAPHICS:
Q18. What is your age?

18-24
25-34
35-44
45-54
55-64
65 or older

Q19. What is your gender?

Male
Female
Non-binary/Other
Prefer not to say

Q20. What is your race/ethnicity? (Select all that apply)

White
Black or African American
Hispanic or Latino
Asian
Native American
Other (please specify)
Prefer not to say

Q21. What is the highest level of education you have completed?

Less than high school
High school graduate
Some college
Associate's degree
Bachelor's degree
Graduate degree
Prefer not to say

Q22. What was your total household income before taxes in 2023?

Under $25,000
$25,000-$49,999
$50,000-$74,999
$75,000-$99,999
$100,000-$149,999
$150,000 or more
Prefer not to say

CONSISTENCY CHECK:
Q23. Looking ahead to November 2024, if Kamala Harris is the Democratic nominee and Donald Trump is the Republican nominee, how do you think you will vote?

Kamala Harris
Donald Trump
Another candidate (please specify)
Would not vote
Not sure

[END OF SURVEY]
Thank you for completing this survey about the 2024 U.S. Presidential Election. Your responses will help us better understand voter preferences and political attitudes across the country.
If you have any questions about this research or would like to be informed about the results, please contact andrew.goh@mail.utoronto.ca.
Your time and participation is greatly appreciated.

# Additional data details

# Model details {#sec-model-details}


## Diagnostics

```{r}
#| label: fig-pen
#| fig-cap: ROC Curve for Harris Win Prediction, Pennsylvania
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Pennsylvania)

```

```{r}
#| label: fig-geo
#| fig-cap: ROC Curve for Harris Win Prediction, Georgia
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Georgia)

```

```{r}
#| label: fig-nor
#| fig-cap: ROC Curve for Harris Win Prediction, North Carolina
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_North_Carolina)

```

```{r}
#| label: fig-mic
#| fig-cap: ROC Curve for Harris Win Prediction, Michigan
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Michigan)

```

```{r}
#| label: fig-ari
#| fig-cap: ROC Curve for Harris Win Prediction, Arizona
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Arizona)

```

```{r}
#| label: fig-wis
#| fig-cap: ROC Curve for Harris Win Prediction, Wisconsin
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Wisconsin)

```

```{r}
#| label: fig-nev
#| fig-cap: ROC Curve for Harris Win Prediction, Nevada
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Nevada)

```

```{r}
#| label: fig-log
#| fig-cap: ROC Curve for Popular Vote Prediction
#| echo: false
#| message: false
#| warning: false

plot(roc_obj)

```

```{r}
#| label: fig-logadj
#| fig-cap: ROC Curve for Popular Vote Prediction, Adjusted
#| echo: false
#| message: false
#| warning: false

plot(roc_obj)

```

\newpage


# References


