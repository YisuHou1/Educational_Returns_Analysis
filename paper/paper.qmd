---
title: "A Logistic Regression Prediction of 2024 Election Odds Gives VP Harris a 38.3% Winrate"
author: 
  - Andrew Goh
  - Yisu Hou
thanks: "Code and data are available at: [https://github.com/YisuHou1/US_Election_Statistics](https://github.com/YisuHou1/US_Election_Statistics)."
date: today
date-format: long
abstract: "The 2024 Presidential Election, run between the Democratic candidate Vice President Kamala Harris and Republican candidate Former President Donald Trump, is both widely predicted to be a close race [@538data] and considered to be the “most important election of your life” by a majority of likely voters this election season, as estimated by Quinnipiac [@quinnipiac]. To predict the results of this election, we use a FiveThirtyEight poll-of-polls dataset [@538data], fitting multinomial logistic regression models with the data both at a national level and for each of the 7 swing states. Using these estimates for the probability of Harris winning the popular votes in each of those categories, we find that Vice President Harris has a 38.3% probability of winning the election in the context of the electoral college, a statistically significant result after considering margin of error and various biases."
format: pdf
header-includes:
  - \usepackage{float}
  - \usepackage{placeins}
number-sections: true
bibliography: references.bib
---

```{r}
#| include: false
#| warning: false
#| message: false

library(tidyverse)
library(caret)
library(glmnet)
library(nnet)
library(lubridate)
library(pROC)
library(stringr)
library(gt)
library(broom)
library(knitr)
```


# Introduction

With most polls maintaining close (<1) odds at both the national and statewide level for the seven swing states [@538data; @nyt2024polls], the 2024 Presidential Elections are proving to be an extremely volatile one, with poll predictions regularly flipping and disagreeing with each other, even up until the day before Election Day. The volatility even extends beyond the polling itself – since the January 6 Capitol Attack, prominent figures supporting both parties have painted the 2024 elections specifically as one that underpins the very democratic integrity of the country. Elon Musk, for example, has openly expressed concerns of a Harris win leading to a one-party country [@musk2024x], and Zack Beauchamp from Vox has described a Trump re-election as an “extinction-level threat to democracy” [@vox]. In short, the stakes are high, at least as perceived in the eyes of voters [@quinnipiac]. 

In order to arrive at such a conclusion, we used poll-by-polls data from FiveThirtyEight [@538data] to fit multinomial logistic regression models for both the nation as a whole and each of the seven swing states. Taking into consideration the mispredictions by pollsters for both the 2016 and 2020 elections, where Trump won the presidential race against Clinton in a 304-227 landslide despite predictions universally predicting a landslide win in the opposite direction [@weighting2], we regress on the characteristics of the poll/pollster such as transparency rating and sponsors and correct the results for systematic polling biases based on past election data, an approach predicted to yield better results this year [@weighting].

Our estimand, or what we seek to predict, is the probability that Kamala Harris wins the election on the 6th of November. 

After obtaining the probabilities that Harris wins each swing state via randomised sampling using our fitted models, we then analysed these probabilities in the context of the electoral college, i.e. the probability that Harris can obtain 44 votes from the swing states, which would all but guarantee her step into office [@nyt2024polls]. We find that Harris has a 38.3% chance of winning the general election, suggesting a likely Trump advantage after accounting for the margin of error in all related processes and observations. 

The body of the paper contains the data, model, results, and discussion sections. In order, @sec-data contains an overview of the data used, the variables involved and the tools used in our analysis and interpretation of the data, @sec-model describes the model in detail and other related miscellaneous musings, @sec-results contains a run-down and explanation of the results and findings from the model, and @sec-discussion contains a discussion of the implications and limitations of the result, among others. The appendices contain a sample of an idealised methodology and survey, as well as a case study of the sample and methodology of Quinnipiac’s Pennsylvania poll [@quinnipiac; @quinnipiacmethodology]. 

# Data {#sec-data}

## Overview

We use the statistical programming language R [@R] for the graphing, analysis and presentation of the project as a whole. Caret [@caret], glmnet [@glmnet; @glmnet2; @glmnet3], nnet [@nnet], lubridate [@lubridate] and pROC [@pROC] were used directly in the analysis of the data. Tidyverse [@tidyverse], stringr [@stringr], dplyr [@dplyr], gt [@gt], knitr [@knitr], and styler [@styler] were used in the presentation and/or styling of the data, graphs, and paper as a whole.

In order to build a model that predicts the results of the 2024 election, we use survey/poll data on likely voters to inform our insights regarding likely election results, an approach widely accepted in political contexts [@politicalpolling]. We use a poll-of-polls dataset from 538 [@538data], consisting of aggregated polling data from FiveThirtyEight's 2024 presidential election polling database from various national and state-level polls from pollsters all around the country, recording information such as the date when the poll concluded, the entity conducting the poll, and the estimated support rate of the candidate as given by the poll. The data spans national and state-level polls, with particular focus on key battleground states that could determine the Electoral College outcome. This dataset was chosen over alternatives like RealClearPolitics because of 538's comprehensive methodological adjustments and transparent quality standards for included polls.

After cleaning the data to split results by candidate, removing polls with N/A values and ratings lower than 2.5, and further selecting polls polling likely voters as opposed to the whole population, we are left with 746 data points with the variables pollster, has_sponsor, numeric_grade, pollscore, transparency_score, sample_size, end_date, state, candidate_name, pct (support rate in percentage), and cycle (election cycle, i.e. 2020/2024).

## Measurement

The "from-voter-to-data" process for the 538 dataset involves three processes: measurement, where voter responses are collected and adjusted to best provide insight into a true support percentage population parameter, data collection, where data from polls is obtained and cleaned by 538 to only include polls adhering to their set of standards, and aggregation, where additional scores/variables are given to each poll to reflect their accuracy, transparency and bias levels with regards to polling scores [@5381].

The measurement process involves several sequential steps, beginning with initial data collection where pollsters conduct surveys using various modes such as phone, online, or mixed-mode approaches to gather voter preferences. Sample processing follows, involving likely voter screens to identify probable voters, demographic weighting to match population benchmarks, and various adjustments to ensure representativeness [@quinnipiac; @newyorktimes].

The data collection process involves obtaining data from various election polls from around the country, given that they meet specific methodological criteria. Each included poll must provide clear documentation of pollster identity, survey dates, and sampled population, maintain a minimum sample size of 100 respondents, and demonstrate transparent methodology including polling mode, sample source, and weighting procedures. Scientific sampling methods attempting to achieve representative samples are mandatory, as is the disclosure of poll sponsorship and funding sources. The dataset explicitly excludes non-scientific polls lacking representative sampling, MRP-smoothed data, recontact surveys, DIY polls from nonprofessional sources, polls with leading questions or hypothetical matchups, and subsamples from multi-state polls without geographic verification [@5382].

The aggregation phase, finally, involves rating polls by pollster polling quality (ranging from 0.5 to 3.0 stars), with polls with higher historical accuracy, lower consistent partisan biases, and higher transparency of methodology boasting higher ratings [@5383]. Transparency is also a rating and is done through a 10-question yes/no checklist on how much of the methodology is available to the public - note how this is completely separate from the quality of the methodology itself. Each question is graded on a 0/0.5/1 scale, for a combined transparency score that ranges from 0 to 10. Most polls in our dataset miss the mark on one or two criteron, netting a final transparency score of 9 - this is partially due to the data cleaning process removing many relatively worsely conducted polls.

Several important measurement limitations to FiveThirtyEight's methodology must be acknowledged. These include temporal gaps between polls creating discontinuous measurement of voter sentiment, response rates and participation biases potentially skewing samples, and varying geographic coverage with swing states being overrepresented. Additionally, third-party candidate treatment varies across polls, and is difficult to standardise (with respect to other data points) consistently.

## Outcome variable

The outcome variable is the level of support for the two leading US presidential candidates, Vice President Kamala Harris and Former President Donald Trump, recorded as a percentage of likely voters. For the national-level analysis, the support percentage represents the percentage of polled individuals who responded in favor of a specific candidate to a question that asked about their preferred presidential candidate in a national-level poll. For the regional datasets, the support percentage represents the percentage of likely voters who preferred a specific candidate in a poll that targets a specific state or congressional district of the United States. @fig-outcome shows the distribution of support percentages in the aggregated polling data from 2020 and 2024 presidential elections.

```{r}
#| echo: false
#| message: false
#| warning: false

trump_2024 <- read_csv(here::here("data/02-analysis_data/trump_2024_analysis_data.csv"))
harris_2024 <- read_csv(here::here("data/02-analysis_data/harris_2024_analysis_data.csv"))
trump_2024_lower <- 
  read_csv(here::here("data/02-analysis_data/trump_2024_analysis_data_lower.csv"))
harris_2024_lower <- 
  read_csv(here::here("data/02-analysis_data/harris_2024_analysis_data_lower.csv"))
trump_2020 <- read_csv(here::here("data/02-analysis_data/trump_2020_analysis_data.csv"))
biden_2020 <- read_csv(here::here("data/02-analysis_data/biden_2020_analysis_data.csv"))

# aggregated dataset for summary statistics
# datasets with lower numeric scores already includes polls with a score of 3.0
all_combined <- bind_rows(harris_2024_lower, trump_2024_lower,
    biden_2020, trump_2020)

```

```{r}
#| label: fig-outcome
#| fig-cap: Percentage Support in US Presidential Polls
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = pct)) +
  geom_histogram(
    binwidth = 1,                # Adjust bin width for granularity
    fill = "#2c7bb6",            # Aesthetic fill color (steel blue)
    color = "white",             # Border color for bins
    alpha = 0.8                   # Transparency of the fill
  ) +
  labs(
    x = "Percentage Support",
    y = "Number of Polls"
  ) +
  theme_minimal(base_size = 14) +   # Minimal theme with increased base font size
  theme(
    plot.title = element_text(
      size = 16, 
      face = "bold", 
      hjust = 0.5,                # Center the title
      margin = margin(b = 10)     # Add space below the title
    ),
    plot.subtitle = element_text(
      size = 12, 
      hjust = 0.5, 
      margin = margin(b = 20)     # Add space below the subtitle
    ),
    axis.title = element_text(face = "bold"), # Bold axis titles
    axis.text = element_text(color = "gray30"), # Dark gray axis text
    panel.grid.major = element_line(color = "gray80"), # Light gray major grid lines
    panel.grid.minor = element_blank()                 # Remove minor grid lines
  ) +
  # Add a vertical line for the mean percentage support
  geom_vline(
    aes(xintercept = mean(pct, na.rm = TRUE)),
    color = "#d73027",           # Aesthetic color (red)
    linetype = "dashed", 
    size = 1
  ) +
  # Annotate the mean value on the plot
  annotate(
    "text",
    x = mean(all_combined$pct, na.rm = TRUE) - 7, # Position text slightly to the right of the mean line
    y = Inf,                                     # Position text at the top of the plot
    label = paste("Mean:", round(mean(all_combined$pct, na.rm = TRUE), 1), "%"),
    vjust = 2,                                   # Vertical adjustment
    color = "#d73027",
    size = 5,
    fontface = "bold"
  )

```

\FloatBarrier

The aggregated polling data includes support percentages of different presidential candidates in polls with different FiveThirtyEight ratings [@5382] across two elections.

As displayed by @fig-outcome, the distribution of support percentages is close to a normal distribution, with most data points between 45 and 52 percent.

## Predictor variables

The predictor variables that we ended up using in our final logistic regression model were the poll end dates, transparency score, and sample sizes of the polls, as well as whether or not a poll was sponsored. The poll end dates were included to capture the temporal changes in candidates' support rate. Transparency score was selected as an identification of the polls' trustworthiness, which must be controlled for in the analysis. Other variables representing the poll quality in the raw dataset, such as the poll score and the FiveThirtyEight rating, were excluded from the models for two reasons. First, the documentation by FiveThirtyEight suggests that the numeric rating, poll score, and transparency score are highly related [@5382]. Second, the data cleaning process filtered data points with high FiveThirtyEight ratings and poll scores. The distributions of selected variables are as follows:

### End Date
```{r}
#| label: fig-dates-past
#| fig-cap: Distribution of Poll End Date, 2020 Cycle
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined %>% filter(cycle == 2020), aes(x = end_date)) +
  geom_histogram(binwidth = 7, fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "End Date",
    y = "Number of Polls"
  ) +
  scale_x_date(date_labels = "%b %d, %Y", date_breaks = "1 month") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
\vspace{1em}
\FloatBarrier

```{r}
#| label: fig-dates-current
#| fig-cap: Distribution of Poll End Date, 2024 Cycle
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined %>% filter(cycle == 2024), aes(x = end_date)) +
  geom_histogram(binwidth = 7, fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "End Date",
    y = "Number of Polls"
  ) +
  scale_x_date(date_labels = "%b %d, %Y", date_breaks = "1 month") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
\FloatBarrier
The end date indicates the date that a poll ends on. The dates are graphed separately between the 2020 and 2024 cycles for ease of viewing. Notice how there is a disproportionately large uptick in the number of polls that end on the end of a month.

### Sponsors

```{r}
#| label: fig-sponsored
#| fig-cap: Number of Polls with and without Sponsors
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = factor(has_sponsor, levels = c(0,1), labels = c("No Sponsor", "Has Sponsor")))) +
  geom_bar(fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "Has Sponsor",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold")
  )

```

Has_sponsor is a categorical variable that keeps track of whether the poll is sponsored by a third party other than the one conducting the poll itself. This is distributed roughly 60-40, with the larger portion of polls being independently funded (i.e. no sponsors).

\FloatBarrier

### Transparency Score

```{r}
#| label: fig-transparency
#| fig-cap: Distribution of Poll Transparency Scores
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = factor(transparency_score))) +
  geom_bar(fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "Transparency Score",
    y = "Number of Polls"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold")
  )

```

The transparency score is a metric given to a poll/pollster by 538 that measures the transparency of the methodology with which the poll is conducted. This is done through a 10-question yes/no checklist on how much of the methodology is available to the public - note how this is completely separate from the quality of the methodology itself. Each question is graded on a 0/0.5/1 scale, for a combined transparency score that ranges from 0 to 10 [@5383]. Most polls in our dataset miss the mark on one or two criteron, netting a final transparency score of 9 - this is partially due to the data cleaning process removing many relatively worsely conducted polls.

\FloatBarrier

### Sample Size

```{r}
#| label: fig-sample
#| fig-cap: Distribution of Poll Sample Sizes
#| echo: false
#| message: false
#| warning: false

ggplot(all_combined, aes(x = sample_size)) +
  geom_histogram(binwidth = 500, fill = "#1f78b4", color = "white", alpha = 0.7) +
  labs(
    x = "Sample Size",
    y = "Number of Polls"
  ) +
  scale_x_continuous(labels = scales::comma) +
  coord_cartesian(xlim = c(0, 6000)) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold")
  )

```

Sample size, finally, is a measure of the number of participants in each poll. Around 1000 participants seems to overwhelmingly be the final survey size that pollsters decide on, with only single-digit numbers of polls (with rating >= 2.5) boasting more than four thousand respondents.

\FloatBarrier

# Model {#sec-model}

The goal of our modelling strategy is twofold. Firstly, we seek to predict the winning candidate on election day by calculating the expected win rate of Vice President Kamala Harris and Former President Donald Trump in the seven swing states: Pennsylvania, Georgia, North Carolina, Michigan, Arizona, Wisconsin, and Nevada [@nyt2024polls]. Secondly, we hope to predict the candidate who has the popular vote on election day, controlling for the potential bias of national polls by accounting for the difference between polled popular vote and the actual popular vote on election day for the 2020 cycle.

To achieve the first goal, the polls dataset was divided by individual states, and only data points from the seven swing states were selected, forming seven smaller datasets. Then, for each state, the harris_win variable was constructed by comparing the support rate of the candidates in each poll. The variable was set to 1 if the support rate of Harris is greater than or equal to Trump's support rate and 0 when Trump's support rate is greater. A logistic model was constructed for each state, using harris_win as the outcome and previously stated predictors as inputs. 

Then, after the models were constructed, we conducted 1000 simulations of election results with randomly-generated parameters. The date parameter was set to election day, and the other three predictor variables were simulated from a normal distribution with the mean and standard deviation of the training dataset. We then averaged the values to obtain our final prediction for the probability of a Harris win in the state of interest.

Then, using the probabilities for each state and their respective electoral college votes, we calculated the probability that Vice President Harris successfully obtains the 44 required electoral college votes to secure the presidency. This is done via an R script that can be found in the end of /scripts/05-model_data.R.

For the popular vote, we utilized the data from 2020 to calculate each pollster's bias by identifying the average difference between the popular vote on election day and their polled national support rates of Trump and President Joe Biden. Next, we adjusted the polled national support rates of pollsters in the 2024 cycle by the calculated gap in 2020. By making this adjustment, we are assuming that the pollsters systematically bias 2024 candidates equally compared to 2020 candidates. We acknowledge that the assumption is most likely false. Ideally, incorporating poll data from earlier election cycles may produce a more accurate indicator of how pollsters systematically bias candidates from the two dominant political parties. However, comprehensive poll data before 2020 is unavailable. Since popular vote is unrelated to the winner of the election, we are using this analysis as an exploration of methods that incorporate historical data without emphasizing predictiveness. Using both adjusted and unadjusted national support levels, multiple linear regressions were created with support levels as the outcome and stated predictors as inputs to predict candidates' support rate over time. Adjusted and unadjusted logistic regressions were made using harris_win as the outcome and the same predictors to predict the candidate with the popular vote on election day. The attempt to control for historical bias was not applied to the swing states due to a lack of data.

## Model set-up

### Logistic models
Define $y_i$ as the binary outcome variable indicating whether Harris wins (1) or not (0) for the $i^{th}$ observation. $x_{1i}$ is the end_date for the $i^{th}$ observation. $x_{2i}$ is the binary value of has_sponsor for the $i^{th}$ observation. $x_{3i}$ is the transparency_score for the $i^{th}$ observation. $x_{4i}$ is the sample_size for the $i^{th}$ observation.

The logistic regression model can be expressed as follows:

\begin{align} y_i &\sim \text{Bernoulli}(p_i) 
\\ \text{logit}(p_i) &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} \end{align}

Line (1) specifies that the outcome variable $y_i$ follows a Bernoulli distribution with probability $p_i$ of success (i.e., Harris winning). Line (2) links the linear combination of predictors to the probability $p_i$, where $\beta_0$ is the intercept term, and $\beta_1, \beta_2, \beta_3, \beta_4$ are coefficients corresponding to each predictor variable above. 

This logistic regression model was applied independently to the data on each swing state and the adjusted and unadjusted popular vote polls. 

### Multiple linear regression models
Define $y_i$ as the percentage national support for Harris or Trump for the $i^{th}$ observation. $x_{1i}$ is the end_date for the $i^{th}$ observation. $x_{2i}$ is the binary value of has_sponsor for the $i^{th}$ observation. $x_{3i}$ is the transparency_score for the $i^{th}$ observation. $x_{4i}$ is the sample_size for the $i^{th}$ observation.

The multiple linear regression model can be expressed as follows:

\begin{align} y_i &= \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \beta_3 x_{3i} + \beta_4 x_{4i} \end{align}

Where $\beta_0$ is the intercept term, and $\beta_1, \beta_2, \beta_3, \beta_4$ are coefficients corresponding to each predictor variable.

Separate multiple linear regression models were applied to the polled national support rate of Harris and Trump, with and without adjustments from historical data. 

### Model justification

We decided to use a logistic model to predict the winner on election day because it directly outputs candidates' chance of winning each swing state, enabling us to use a probabilistic function to identify their overall chance of winning. Comparatively, a linear regression model merely predicts the level of support for candidates, which fails to identify a winner. 

```{r}
#| echo: false
#| message: false
#| warning: false

# Regional polls analysis
trump_2024_regional <- trump_2024_lower %>% filter(state != "National")
harris_2024_regional <- harris_2024_lower %>% filter(state != "National")


trump_Pennsylvania <- trump_2024_regional %>% filter(state == "Pennsylvania")
trump_Georgia <- trump_2024_regional %>% filter(state == "Georgia")
trump_North_Carolina <- trump_2024_regional %>% filter(state == "North Carolina")
trump_Michigan <- trump_2024_regional %>% filter(state == "Michigan")
trump_Arizona <- trump_2024_regional %>% filter(state == "Arizona")
trump_Wisconsin <- trump_2024_regional %>% filter(state == "Wisconsin")
trump_Nevada <- trump_2024_regional %>% filter(state == "Nevada")

harris_Pennsylvania <- harris_2024_regional %>% filter(state == "Pennsylvania")
harris_Georgia <- harris_2024_regional %>% filter(state == "Georgia")
harris_North_Carolina <- harris_2024_regional %>% filter(state == "North Carolina")
harris_Michigan <- harris_2024_regional %>% filter(state == "Michigan")
harris_Arizona <- harris_2024_regional %>% filter(state == "Arizona")
harris_Wisconsin <- harris_2024_regional %>% filter(state == "Wisconsin")
harris_Nevada <- harris_2024_regional %>% filter(state == "Nevada")

combined_Pennsylvania <- full_join(
  trump_Pennsylvania,
  harris_Pennsylvania,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Georgia <- full_join(
  trump_Georgia,
  harris_Georgia,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_North_Carolina <- full_join(
  trump_North_Carolina,
  harris_North_Carolina,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Michigan <- full_join(
  trump_Michigan,
  harris_Michigan,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Arizona <- full_join(
  trump_Arizona,
  harris_Arizona,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Wisconsin <- full_join(
  trump_Wisconsin,
  harris_Wisconsin,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_Nevada <- full_join(
  trump_Nevada,
  harris_Nevada,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

# mutate harris_win variable by comparing polled support rates
# preparing data for logistic regression
combined_Pennsylvania <- combined_Pennsylvania %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Georgia <- combined_Georgia %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_North_Carolina <- combined_North_Carolina %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Michigan <- combined_Michigan %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Arizona <- combined_Arizona %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Wisconsin <- combined_Wisconsin %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

combined_Nevada <- combined_Nevada %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

# Create 7 different logistic regressions to predict Harris's win rate
# in each state
logistic_Pennsylvania <- readRDS(here::here("models/Logistic_model_Pennsylvania.rds"))

logistic_Georgia <- readRDS(here::here("models/Logistic_model_Georgia.rds"))

logistic_North_Carolina <- readRDS(here::here("models/Logistic_model_North_Carolina.rds"))

logistic_Michigan <- readRDS(here::here("models/Logistic_model_Michigan.rds"))

logistic_Arizona <- readRDS(here::here("models/Logistic_model_Arizona.rds"))

logistic_Wisconsin <- readRDS(here::here("models/Logistic_model_Wisconsin.rds"))

logistic_Nevada <- readRDS(here::here("models/Logistic_model_Nevada.rds"))

# test logistic models for state specific analysis
# mutate fitted values

combined_Pennsylvania <- combined_Pennsylvania %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Pennsylvania, type = "response"), 2)
  )

roc_obj_Pennsylvania <- roc(combined_Pennsylvania$harris_win,
                  combined_Pennsylvania$harris_win_prob)

auc_value_Pennsylvania <- auc(roc_obj_Pennsylvania)


combined_Georgia <- combined_Georgia %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Georgia, type = "response"), 2)
  )

roc_obj_Georgia <- roc(combined_Georgia$harris_win,
                            combined_Georgia$harris_win_prob)

auc_value_Georgia <- auc(roc_obj_Georgia)


combined_North_Carolina <- combined_North_Carolina %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_North_Carolina, type = "response"), 2)
  )

roc_obj_North_Carolina <- roc(combined_North_Carolina$harris_win,
                            combined_North_Carolina$harris_win_prob)

auc_value_North_Carolina <- auc(roc_obj_North_Carolina)


combined_Michigan <- combined_Michigan %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Michigan, type = "response"), 2)
  )

roc_obj_Michigan <- roc(combined_Michigan$harris_win,
                            combined_Michigan$harris_win_prob)

auc_value_Michigan <- auc(roc_obj_Michigan)

combined_Arizona <- combined_Arizona %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Arizona, type = "response"), 2)
  )

roc_obj_Arizona <- roc(combined_Arizona$harris_win,
                            combined_Arizona$harris_win_prob)

auc_value_Arizona <- auc(roc_obj_Arizona)


combined_Wisconsin <- combined_Wisconsin %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Wisconsin, type = "response"), 2)
  )

roc_obj_Wisconsin <- roc(combined_Wisconsin$harris_win,
                            combined_Wisconsin$harris_win_prob)

auc_value_Wisconsin <- auc(roc_obj_Wisconsin)


combined_Nevada <- combined_Nevada %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Nevada, type = "response"), 2)
  )

roc_obj_Nevada <- roc(combined_Nevada$harris_win,
                            combined_Nevada$harris_win_prob)

auc_value_Nevada <- auc(roc_obj_Nevada)

```

We identified the AUC values and ROC plots for each of the seven swing-state logistic regressions. The ROC plots are presented in the Additional Data Details section (@sec-model-details), from @fig-pen to @fig-nev. The AUC values are: 0.90 for Pennsylvania, 0.88 for Georgia, 0.74 for North Carolina, 0.93 for Michigan, 1.00 for Arizona, 0.86 for Wisconsin, and 0.65 for Nevada. All states other than Nevada have acceptable or excellent AUC values, indicating that the respective logistic models are reliable when distinguishing cases where Harris wins. The low AUC value for Nevada is potentially caused by the limited sample size; there are only 9 polls conducted in Nevada.

Moreover, the sample size restrictions were the reason why we did not use training and testing data to validate our logistic models. The state with the greatest number of polls, Pennsylvania, only records 26 polls. Utilizing training and testing datasets implies that the datasets may include only a couple of data points in some states, which is insufficient to construct or test the model. On top of that, according to [@trainingerror], achieving low training error, such as by using a training-MSE-minimising regression model such as our own, is sufficient to ensure the predictive capabilities of a model bounded by some constant variance term.

```{r}
#| echo: false
#| message: false
#| warning: false

# find biases of pollsters from 2020 national polls data
# filter out national polls from the datasets
trump_2024_national <- trump_2024 %>% filter(state == "National")
harris_2024_national <- harris_2024 %>% filter(state == "National")
trump_2020_national <- trump_2020 %>% filter(state == "National")
biden_2020_national <- biden_2020 %>% filter(state == "National")

# Define actual results
actual_results_2020 <- list(Trump_pct = 47, Biden_pct = 51)

# Calculate bias per pollster
rep_pollster_bias <- trump_2020_national %>%
  group_by(pollster) %>%
  summarize(
    Trump_bias = mean(pct - actual_results_2020$Trump_pct, na.rm = TRUE)
  )

dem_pollster_bias <- biden_2020_national %>%
  group_by(pollster) %>%
  summarize(
    Biden_bias = mean(pct - actual_results_2020$Biden_pct, na.rm = TRUE)
  )

# adjust 2024 national polls percentages by the average pollster biases
# also mutate has_sponsor variable into dummy variable and remove NA's
trump_2024_national_adj <- trump_2024_national %>%
  left_join(rep_pollster_bias, by = "pollster") %>%
  mutate(
    pct_adj = pct - ifelse(!is.na(Trump_bias), Trump_bias, 0)
  ) %>%
  select(-Trump_bias)

# Adjust Harris 2024 National Polls
harris_2024_national_adj <- harris_2024_national %>%
  left_join(dem_pollster_bias, by = "pollster") %>%
  mutate(
    pct_adj = pct - ifelse(!is.na(Biden_bias), Biden_bias, 0)
  ) %>%
  select(-Biden_bias)


# model adjusted support percentage by time and control variables
# first split training and testing dataset using an 80/20 split
set.seed(11451)
trainIndex_trump_adj <- createDataPartition(trump_2024_national_adj$pct_adj, 
                                        p = 0.8, list = FALSE)
data_train_trump_adj <- trump_2024_national_adj[trainIndex_trump_adj, ]
data_test_trump_adj  <- trump_2024_national_adj[-trainIndex_trump_adj, ]

trainIndex_harris_adj <- createDataPartition(harris_2024_national_adj$pct_adj, 
                                         p = 0.8, list = FALSE)
data_train_harris_adj <- harris_2024_national_adj[trainIndex_harris_adj, ]
data_test_harris_adj  <- harris_2024_national_adj[-trainIndex_harris_adj, ]

model_date_harris_adj <- lm(pct_adj ~ end_date + has_sponsor +
                          transparency_score + sample_size, data = data_train_harris_adj)
model_date_trump_adj <- lm(pct_adj ~ end_date + has_sponsor +
                         transparency_score + sample_size, data = data_train_trump_adj)

# repeat for unadjusted support levels
trainIndex_trump <- createDataPartition(trump_2024_national$pct, 
                                        p = 0.8, list = FALSE)
data_train_trump <- trump_2024_national[trainIndex_trump, ]
data_test_trump  <- trump_2024_national[-trainIndex_trump, ]

trainIndex_harris <- createDataPartition(harris_2024_national$pct, 
                                         p = 0.8, list = FALSE)
data_train_harris <- harris_2024_national[trainIndex_harris, ]
data_test_harris  <- harris_2024_national[-trainIndex_harris, ]

model_date_harris <- lm(pct ~ end_date + has_sponsor +
                          transparency_score + sample_size, data = data_train_harris)
model_date_trump <- lm(pct ~ end_date + has_sponsor +
                         transparency_score + sample_size, data = data_train_trump)


# Functions to evaluate model using testing dataset
evaluate_model_adj <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data)
  actuals <- test_data$pct_adj
  rmse <- sqrt(mean((predictions - actuals)^2, na.rm = TRUE))
  r_squared <- summary(model)$r.squared
  return(list(RMSE = rmse, R_squared = r_squared))
}

evaluate_model <- function(model, test_data) {
  predictions <- predict(model, newdata = test_data)
  actuals <- test_data$pct
  rmse <- sqrt(mean((predictions - actuals)^2, na.rm = TRUE))
  r_squared <- summary(model)$r.squared
  return(list(RMSE = rmse, R_squared = r_squared))
}

# Evaluate Harris model with adjusted support percentages
harris_evaluation_adj <- evaluate_model_adj(model_date_harris_adj, data_test_harris_adj)


# Evaluate Trump model with adjusted support percentages
trump_evaluation_adj <- evaluate_model_adj(model_date_trump_adj, data_test_trump_adj)


# Evaluate Harris model with unadjusted support percentages
harris_evaluation <- evaluate_model(model_date_harris, data_test_harris)


# Evaluate Trump model with unadjusted support percentages
trump_evaluation <- evaluate_model(model_date_trump, data_test_trump)


# Augment data with model predictions
model_harris_adj <- readRDS(here::here("models/MLR_adjusted_harris.rds"))
model_trump_adj <- readRDS(here::here("models/MLR_adjusted_trump.rds"))

harris_2024_national_adj <- harris_2024_national_adj %>% mutate(
  fitted_date = predict(model_harris_adj)
)

trump_2024_national_adj <- trump_2024_national_adj %>% mutate(
  fitted_date = predict(model_trump_adj)
)

# Unadjusted datasets
model_harris <- readRDS(here::here("models/MLR_harris.rds"))
model_trump <- readRDS(here::here("models/MLR_trump.rds"))

harris_2024_national <- harris_2024_national %>% mutate(
  fitted_date = predict(model_harris)
)

trump_2024_national <- trump_2024_national %>% mutate(
  fitted_date = predict(model_trump)
)

# evaluate models without using training and testing data
harris_eval_adj <- evaluate_model_adj(model_harris_adj, harris_2024_national_adj)


harris_eval <- evaluate_model(model_harris, harris_2024_national)


trump_eval_adj <- evaluate_model_adj(model_trump_adj, trump_2024_national_adj)


trump_eval <- evaluate_model(model_trump, trump_2024_national)



#### Logistic regression for national support ####

combined_national <- full_join(
  trump_2024_national,
  harris_2024_national,
  by = c("pollster", "has_sponsor", "pollscore", 
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_national <- combined_national %>% mutate(
  harris_win = case_when(
    pct.y >= pct.x ~ 1,
    pct.y < pct.x ~ 0
  )
)

# repeat for adjusted support levels
combined_national_adj <- full_join(
  trump_2024_national_adj,
  harris_2024_national_adj,
  by = c("pollster", "has_sponsor", "pollscore",
         "transparency_score", "sample_size", "end_date", "state", "cycle")
)

combined_national_adj <- combined_national_adj %>% mutate(
  harris_win = case_when(
    pct_adj.y >= pct_adj.x ~ 1,
    pct_adj.y < pct_adj.x ~ 0
  )
)


# logistic model to predict the popular vote
model_logistic <- readRDS(here::here("models/Logistic_model.rds"))

model_logistic_adj <- readRDS(here::here("models/Logistic_model_adjusted.rds"))


# include predicted outcomes to the dataframes
combined_national <- combined_national %>%
  mutate(
    harris_win_prob = round(100*predict(model_logistic, type = "response"), 2)
  )

combined_national_adj <- combined_national_adj %>%
  mutate(
    harris_win_prob = round(100*predict(model_logistic_adj, 
                                        type = "response"), 2)
  )


#### Evaluate the Model ####

# ROC Curve and AUC
roc_obj <- roc(combined_national$harris_win,
               combined_national$harris_win_prob)
auc_value <- auc(roc_obj)

roc_obj_adj <- roc(combined_national_adj$harris_win,
               combined_national_adj$harris_win_prob)
auc_value_adj <- auc(roc_obj_adj)
```
For the analysis of the national popular vote, the logistic model of adjusted polls possess an AUC value of 0.98, and the logistic model of unadjusted polls have an AUC of 0.94, indicating high reliability of categorization. Their respective ROC plots are presented as @fig-log and @fig-logadj. 

RMSE values and R squared values were calculated for the multiple linear regression models predicting popular vote. The RMSE values for all four models, adjusted and unadjusted support levels of the two candidates, are greater than 1, indicating that the fitted values and the support levels of actual polls are more than 1 percent apart on average, which is significant when analyzing popular vote. Moreover, all R squared values other than the model for the adjusted support rate of Trump, are smaller than 0.3, indicating the inability of the models to account for the variance of national support rates in the data. The model for the adjusted support rate of Trump has a R squared value of 0.69, indicating a relatively high proportion of variance in polled support rates being explained by the multiple linear regression model. This was expected, as the predictors such as a poll's end date and transparency does not necessarily correlate with the outcome of the polls. More predictive variables used by FiveThirtyEight [@5383] for their predictions are unavailable in the raw dataset, which is a key limitation of the data.


# Results {#sec-results}

## Swing states

@tbl-result shows the average likelihood percentage of Harris winning each swing state, calculated by using the logistic model on 1,000 hypothetical data points for each state, generated with the normal distribution of predictors and setting the end_date to election day. It indicates that Harris has a greater probability to win in Pennsylvania, Georgia, and Nevada, while Trump is more likely to win in North Carolina, Michigan, Arizona, and Wisconsin. The electoral votes in each state is also presented.

```{r}
#| echo: false
#| message: false
#| warning: false

#### Results ####
# To predict the outcome on election day, generate 1000 hypothetical data points
# using the election day as end_date and the normal curve of other input 
# variables, mimicing all the situations that may happen on election day
hypothetical_data_Pennsylvania <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_Pennsylvania$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_Pennsylvania$transparency_score), 
                             sd = sd(combined_Pennsylvania$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_Pennsylvania$sample_size),
                      sd = sd(combined_Pennsylvania$sample_size))
)

hypothetical_data_Georgia <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_Georgia$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_Georgia$transparency_score), 
                             sd = sd(combined_Georgia$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_Georgia$sample_size),
                      sd = sd(combined_Georgia$sample_size))
)

hypothetical_data_North_Carolina <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_North_Carolina$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_North_Carolina$transparency_score), 
                             sd = sd(combined_North_Carolina$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_North_Carolina$sample_size),
                      sd = sd(combined_North_Carolina$sample_size))
)

hypothetical_data_Michigan <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_Michigan$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_Michigan$transparency_score), 
                             sd = sd(combined_Michigan$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_Michigan$sample_size),
                      sd = sd(combined_Michigan$sample_size))
)

hypothetical_data_Arizona <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_Arizona$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_Arizona$transparency_score), 
                             sd = sd(combined_Arizona$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_Arizona$sample_size),
                      sd = sd(combined_Arizona$sample_size))
)

hypothetical_data_Wisconsin <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_Wisconsin$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_Wisconsin$transparency_score), 
                             sd = sd(combined_Wisconsin$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_Wisconsin$sample_size),
                      sd = sd(combined_Wisconsin$sample_size))
)

hypothetical_data_Nevada <- tibble(
  end_date = as.Date("2024-11-05"),
  has_sponsor = rbinom(1000, size = 1, prob = mean(combined_Nevada$has_sponsor)),
  transparency_score = rnorm(1000, mean = mean(combined_Nevada$transparency_score), 
                             sd = sd(combined_Nevada$transparency_score)),
  sample_size = rnorm(1000, mean = mean(combined_Nevada$sample_size),
                      sd = sd(combined_Nevada$sample_size))
)

# run the corresponding logistic regression on the hypothetical data of each
# state and average the outcomes to find the expected win rate of Harris
# on election day
hypothetical_data_Pennsylvania <- hypothetical_data_Pennsylvania %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Pennsylvania, newdata = ., type = "response"), 2)
  )


hypothetical_data_Georgia <- hypothetical_data_Georgia %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Georgia, newdata = ., type = "response"), 2)
  )


hypothetical_data_North_Carolina <- hypothetical_data_North_Carolina %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_North_Carolina, newdata = ., type = "response"), 2)
  )


hypothetical_data_Michigan <- hypothetical_data_Michigan %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Michigan, newdata = ., type = "response"), 2)
  )


hypothetical_data_Arizona <- hypothetical_data_Arizona %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Arizona, newdata = ., type = "response"), 2)
  )


hypothetical_data_Wisconsin <- hypothetical_data_Wisconsin %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Wisconsin, newdata = ., type = "response"), 2)
  )


hypothetical_data_Nevada <- hypothetical_data_Nevada %>%
  mutate(
    harris_win_prob = round(100*predict(logistic_Nevada, newdata = ., type = "response"), 2)
  )


# produce a table of mean winning probabilities
mean_probabilities <- tibble(
  State = c("Pennsylvania", "Georgia", "North Carolina", "Michigan", "Arizona", "Wisconsin", "Nevada"),
  Harris_Win_Probability = c(
    round(mean(hypothetical_data_Pennsylvania$harris_win_prob, na.rm = TRUE), 2),
    round(mean(hypothetical_data_Georgia$harris_win_prob, na.rm = TRUE), 2),
    round(mean(hypothetical_data_North_Carolina$harris_win_prob, na.rm = TRUE), 2),
    round(mean(hypothetical_data_Michigan$harris_win_prob, na.rm = TRUE), 2),
    round(mean(hypothetical_data_Arizona$harris_win_prob, na.rm = TRUE), 2),
    round(mean(hypothetical_data_Wisconsin$harris_win_prob, na.rm = TRUE), 2),
    round(mean(hypothetical_data_Nevada$harris_win_prob, na.rm = TRUE), 2)
  )
)


# include electoral votes to determine who is in the lead
state_evs <- tibble::tribble(
  ~State, ~electoral_votes,
  "Alaska", 3,
  "Arizona", 11,
  "Arkansas", 6,
  "California", 55,
  "Colorado", 9,
  "Connecticut", 7,
  "Florida", 29,
  "Georgia", 16,
  "Illinois", 20,
  "Indiana", 11,
  "Iowa", 6,
  "Kansas", 6,
  "Louisiana", 8,
  "Maine", 2,
  "Maine CD-1", 1,
  "Maine CD-2", 1,
  "Maryland", 10,
  "Massachusetts", 11,
  "Michigan", 16,
  "Minnesota", 10,
  "Mississippi", 6,
  "Missouri", 10,
  "Montana", 3,
  "Nebraska", 2,
  "Nebraska CD-2", 1,
  "Nevada", 6,
  "New Hampshire", 4,
  "New Mexico", 5,
  "New York", 29,
  "North Carolina", 15,
  "Ohio", 18,
  "Oklahoma", 7,
  "Oregon", 7,
  "Pennsylvania", 20,
  "Rhode Island", 4,
  "South Carolina", 9,
  "South Dakota", 3,
  "Texas", 38,
  "Utah", 6,
  "Vermont", 3,
  "Virginia", 13,
  "Washington", 12,
  "Wisconsin", 10
)

mean_probabilities <- mean_probabilities %>%
  left_join(state_evs, by = "State")

#### Overall Probability of Winning ####
# Harris
# Create data frame of states
states <- data.frame(
  state = c("PA", "GA", "NC", "MI", "WI", "NV", "AZ"),
  prob = c(0.5551, 0.6801, 0.1033, 0.5151, 0.3792, 0.7679, 0),
  votes = c(20, 16, 15, 16, 10, 6, 11)
)

# Function to calculate probability of a specific combination
calc_combo_prob <- function(combo, states) {
  prob <- 1
  for(i in 1:nrow(states)) {
    if(i %in% combo) {
      prob <- prob * states$prob[i]
    } else {
      prob <- prob * (1 - states$prob[i])
    }
  }
  return(prob)
}

# Generate all possible combinations
n_states <- nrow(states)
total_prob <- 0

# Check each possible combination
for(i in 1:(2^n_states - 1)) {
  # Convert number to binary to get combination
  combo <- which(intToBits(i)[1:n_states] == 1)
  
  # Calculate total electoral votes for this combination
  votes <- sum(states$votes[combo])
  
  # If this combination has enough votes, add its probability
  if(votes >= 44) {
    prob <- calc_combo_prob(combo, states)
    total_prob <- total_prob + prob
  }
}

# Trump
# Create data frame of states
states <- data.frame(
  state = c("PA", "GA", "NC", "MI", "WI", "NV","AZ"),
  prob = c(1-0.5551, 1-0.6801, 1-0.1033, 1-0.5151, 1-0.3792, 1-0.7679, 1-0), # Using 1-p for Trump's probabilities
  votes = c(20, 16, 15, 16, 10, 6, 11)
)

# Function to calculate probability of a specific combination
calc_combo_prob <- function(combo, states) {
  prob <- 1
  for(i in 1:nrow(states)) {
    if(i %in% combo) {
      prob <- prob * states$prob[i]
    } else {
      prob <- prob * (1 - states$prob[i])
    }
  }
  return(prob)
}

# Generate all possible combinations
n_states <- nrow(states)
total_prob <- 0

# Check each possible combination
for(i in 1:(2^n_states - 1)) {
  # Convert number to binary to get combination
  combo <- which(intToBits(i)[1:n_states] == 1)
  
  # Calculate total electoral votes for this combination
  votes <- sum(states$votes[combo])
  
  # If this combination has enough votes, add its probability
  if(votes >= 51) {
    prob <- calc_combo_prob(combo, states)
    total_prob <- total_prob + prob
  }
}


```


```{r}
#| label: tbl-result
#| tbl-cap: Predicted Harris Win Percentage in Swing States
#| echo: false
#| message: false
#| warning: false

mean_probabilities %>%
  gt() %>%
  fmt_number(
    columns = vars(Harris_Win_Probability),
    decimals = 2
  ) %>%
  cols_label(
    State = "State",
    Harris_Win_Probability = "Harris Win Probability (%)",
    electoral_votes = "Electoral Votes"
  ) %>%
  tab_options(
    table.font.size = 12,
    heading.title.font.size = 16,
    heading.subtitle.font.size = 12,
    table.border.top.width = px(2),
    table.border.bottom.width = px(2)
  ) 

```
Combining with the fact that Harris requires 44 electoral votes to win and Trump requires 51, we have found using the probability function that Harris has a 38.3 chance to acquire more than 44 electoral votes, according to the probabilities and electoral votes in @tbl-result. On the other hand, Trump has a 61.7 probability to acquire more than 51 votes. Thus, our conclusion is that while both candidates possess a significant chance of winning, Trump has the greater probability of acquiring the necessary electoral votes from the seven swing states on election day.


## Popular vote

@fig-lradj presents the fitted values of the candidates' adjusted national support rates as well as the data points of polled support rates adjusted for historical pollster bias. It indicates that Trump is expected to have higher national support levels, but the two converges as election day approaches. This result is counter intuitive, as most national polls indicate that Harris will have the popular vote. However, the adjusted values for Trump's support level are significantly greater than the polled values because historically, the actual percentage of the popular vote for Trump in 2020 was significantly greater than his polled percentages of the popular vote. Thus, assuming that the 2024 national polls are equally biased against Trump compared to the 2020 polls, it is difficult to tell who will get the popular vote on election day. However, as previously stated, this assumption is likely false, as the candidates have changed.

```{r}
#| label: fig-lradj
#| fig-cap: Adjusted Popular Vote Prediction, 2024
#| echo: false
#| message: false
#| warning: false
# plot model predictions
# combine the datasets to create a single visual
combined_data_adj <- bind_rows(harris_2024_national_adj, trump_2024_national_adj)
combined_data <- bind_rows(harris_2024_national, trump_2024_national)

# create the plot that contains scattered points of adjusted percentages
# and lines for fitted values
ggplot(combined_data_adj, aes(x = end_date, y = pct_adj, color = candidate_name)) +
  # Scatter points for actual adjusted percentages
  geom_point(alpha = 0.6) +
  # Regression lines
  geom_line(aes(y = fitted_date), size = 0.5) +
  xlim(as.Date("2024-07-19"), as.Date("2024-10-25")) +
  # Assign specific colors
  scale_color_manual(values = 
                       c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  # Customize the plot appearance
  theme_classic() +
  labs(
    y = "Adjusted Percent Support",
    x = "End Date",
    color = "Candidate"
  ) +
  theme(
    text = element_text(size = 12),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )

```
\FloatBarrier

@fig-lr Presents the predicted unadjusted national support rates for Harris and Trump. The conclusions one can make from this plot is similar to those of most polls: Harris is expected to win the popular vote on any day, including the election day. 

```{r}
#| label: fig-lr
#| fig-cap: Unadjusted Popular Vote Prediction, 2024
#| echo: false
#| message: false
#| warning: false
# Unadjusted support percentages
ggplot(combined_data, aes(x = end_date, y = pct, color = candidate_name)) +
  # Scatter points for actual adjusted percentages
  geom_point(alpha = 0.6) +
  # Regression lines
  geom_line(aes(y = fitted_date), size = 0.5) +
  xlim(as.Date("2024-07-19"), as.Date("2024-10-25")) +
  # Assign specific colors
  scale_color_manual(values = 
                       c("Kamala Harris" = "blue", "Donald Trump" = "red")) +
  # Customize the plot appearance
  theme_classic() +
  labs(
    y = "Adjusted Percent Support",
    x = "End Date",
    color = "Candidate"
  ) +
  theme(
    text = element_text(size = 12),
    plot.title = element_text(hjust = 0.5, face = "bold")
  )


```
\FloatBarrier

The regression coefficients for the logistic and linear models are presented in the Additional Data Details section (@sec-model-details) because they are not the outcome of interest.



# Discussion {#sec-discussion   }

## The illusion of polarization 

As mentioned in the introduction, one thing that stands out about the 2024 election in particular is just how polarized the two factions are. Instead of merely being a choice over which individual to helm the country over the next four years, this particular election is being framed more and more as a “fight to protect our democracy” from both sides of the debate stand. This, while evidently a new level of polarization, merely mirrors the trend in recent years more than anything else: party loyalty has been on a steady uprise since 1972, with loyalty in weak partisan and leaning independent voters increasing by approximately 50% in the 40 years that followed [@partisanship]. New records were further set in 2016, with Republican validated voters voting Trump a staggering 92% to 4% for Clinton, and Democratic validated voters voting Clinton an equally extreme 94% to 5% Trump support [@pew2018voters]. 

New voting statistics, however, suggest that this polarization is likely only more prominent in the upper echelons of political representation, however: a Washington Post article found that approximately 1 in 8 women and 1 in 10 men are now voting differently from their spouses compared to before [@wapo2024women], a development that suggests a decrease in long-increasing partisanship trends. This is especially pronounced since households in the States overwhelmingly vote together: at least 70% of them do, at least, which means that this value is projected to decrease from 70% to approximately 62% [@households]. In short: while it seems that ideologies are shifting more and more away from each other, the stats say otherwise. So does the Overton window [@overton]. 

## Win probabilities versus support ratings: why our predictions are as extreme as they are

Our model led to a win probability prediction with numbers much more skewed towards one side than most pollsters currently predict: under this model, we find that Harris has a 40-60 chance against Trump, whereas most pollsters nowadays hover somewhere between 50-50 to 45-55 (towards either side). This is mainly a consequence of using a multinomial logistic regression model as opposed than other models – instead of predicting the support rates of the two candidates, such a model outputs the probability that a random support rate value from the predicted distribution of Harris support rates is larger than that of the predicted distribution of Trump support rates. This means that in states such as Nevada, where 538 gives Trump a +3 advantage, factoring in for margin of error and standard error means that the probability of Harris winning the state is not in fact 47% but rather much closer to zero, which we decided is a more intuitive way of understanding election odds. 

A final reason for this is how swing states are polled much more sample-size-wise than other states, leading to a much higher power value than polls from said other states – a +3 lead in a sample of 1000 people from California’s 39 million might not mean too much, but a +3 lead in a sample of 3000 people from Nevada’s 3 million represents the actual results for 0.3% of the state’s population, holding much more relative weight when it comes to election predictions. 

## Weaknesses and next steps

The dataset used for this analysis has a number of clear limitations. First of all, there is a lack of high-quality polls. Though FiveThirtyEight distributes each pollster with a rating out of 3.0, almost all pollsters under the rating of 2.7 produces outcomes that are unreliable and highly volatile. For this research, it was only possible for us to use pollsters with a 3.0 rating when analyzing national polls; we had to lower the standards to 2.5 for the analysis of swing states, as the number of high-quality state-specific polls are very limited. The inclusion of unreliable data reduces the predictability of our logistic models and thus our final outcome. The lack of predictability of our Nevada model is a perfect example of how low-quality data and the lack of data points impacts our ability to create effective models. However, we acknowledge that this is beyond the control of FiveThirtyEight, as they do not conduct the polls.

The second major challenge presented by the dataset is the lack of predictors. FiveThirtyEight utilizes predictive variables that are unavailable to us, such as weighted values that represent the extent to which a pollster is partisan [@5383]. The predictors used in this research, such as the transparency score of the pollster, are not related to their bias towards any candidate from a partisanship point of view, limiting the predictive power of our models.

Finally, the lack of data regarding the Harris-Trump race, due to Harris starting her presidential race uncharacteristically late, is also a main reason for the limitations of our model. 

Potential next steps could be taken to improve the model in the future: more data could be aggregated manually (from pollsters not recorded by FiveThirtyEight), stronger predictor variables that are specifically more correlated with partisanship could be marked and utilised in our model, and individual weightings could be utilised and applied to the data based on each pollster's "track record" with regards to the error, bias, and variance in their previous polls and estimates.

\newpage

\appendix

# Appendix {-}
## Pollster methodology overview and evaluation
The Quinnipiac University Poll conducts independent polling in swing states. To analyze their methodology, we look specifically into their October 2024 Pennsylvania polls [@quinnipiac].

Their target population is likely voters aged 18 and older in Pennsylvania [@quinnipiacmethodology]. To reach this population, they use likely voters aged 18 and older with phone numbers (both landline and cell) as their sampling frame, i.e. the frame of possible subjects that they sample observations from. Quinnipiac University employed Random Digit Dialing (RDD) to generate their sample of 2,186 respondents. This dual-frame approach reflects modern communication patterns, with 1,644 cell phone and 542 landline completions. However, using phones as a sampling frame means they cannot reach voters without phone access, introducing potential coverage bias.

Their sampling approach uses stratification by Census division according to area code, meaning that they divide Pennsylvania into geographic regions before using RDD to sample within each region. This strategy ensures even geographic representation but adds complexity to the sampling process [@pollofpolls]. For each selected number, they attempt contact at least three times before marking it as non-responsive. For landline calls, they ask to speak with the household member who has the next birthday, a simple but effective randomization technique. Afterwards, a series of screening questions confirm that the subject is indeed a likely voter, after which the subject's responses are then formally taken as part of the sample.

After collecting responses, Quinnipiac adjusts their data through post-stratification weighting. In this weighting, they compare their sample's demographic composition to known population benchmarks from the Census (like age, gender, education, and race distributions in Pennsylvania) and adjust the weight given to each response to match these benchmarks. For example, if their sample has too few young voters compared to Census data, responses from young voters would be weighted more heavily. While this helps correct for sampling imbalances, it can increase the variance in their estimates if the weights vary substantially [@poststratification].

The survey administration addresses measurement issues through its design. Live interviewers conduct all interviews, enabling question clarification and generating higher response rates compared to automated systems. However, live interviewers may introduce social desirability bias, where respondents might modify their answers to appear more socially acceptable. This becomes particularly relevant in political polling, where respondents might hesitate to express unpopular political views [@politicalpolling].

Several types of bias affect the poll's results. Self-selection bias occurs because certain types of people (typically those more politically engaged or with stronger views) are more likely to agree to participate in the survey. Non-response bias arises when people who respond differ systematically from those who don't - for instance, busier people might be less likely to answer calls, potentially underrepresenting certain occupational groups [@nonresponse]. Coverage bias means some groups (like those without phones) have no chance of being included in the sample.

Ethics-wise, the survey does a good job of informing individuals of the details of the survey, such as its purpose and how participants' data is to be used. Personally-identifiable data (i.e. name, phone number, etc.) is not collected during the survey other than to ensure that repeat numbers are not drawn, and the usage of live interviewers to conduct the survey ensures that concerns about the ethics of the survey can be voiced and answered on-the-spot. Combined with the lack of financial incentive for participants, which suggests that all participants participate of their complete free will and intention, this is an ethically sound methodology for conducting a political survey.

Finally, the poll also faces common challenges in political polls such as this one - the five-day field period (October 24-28) may miss opinion changes close to election day, and while weighting adjustments help correct for demographic imbalances, they may increase variance in the estimates if some groups need to be weighted heavily to match population benchmarks, for example.

Overall, Quinnipiac's methodology represents a balanced approach to managing practical constraints and statistical rigor in modern political polling - while some common biases are still likely to skew the poll results off the true support levels for Harris, for example, the poll uses methods such as post-stratification weighting to tradeoff biases at the cost of model variance [@weighting]. Using live interviewers unavoidably introduces social desirability bias, however, and significantly increases poll costs per quota. Modifying the methodology to remove this aspect of the survey would potentially reduce bias and allow for larger samples to be taken, in turn opening up possibilities for cross-validation and the such, which then reduces the effect of increased model variance on the final results.

## Idealised methodology
With a $100,000 budget, our approach focuses on producing accurate state-level estimates in key battleground states, which would then inform our national forecast. We prioritize Pennsylvania, Michigan, Wisconsin, Georgia, Arizona, and Nevada, allocating resources proportionally based on each state's electoral importance and expected margin of victory.

Our sampling strategy employs both probability and non-probability methods. Probability sampling (60% of budget, i.e. $60k) means every member of our target population has a known, non-zero chance of being selected - this allows us to calculate proper margins of error and make statistical inferences about the population. For this, we use dual-frame random digit dialing (RDD) for phone surveys and address-based sampling (ABS) for mail-to-web recruitment. RDD involves generating random phone numbers within active area codes, while ABS uses the U.S. Postal Service's delivery database as a sampling frame, both proven methods of sampling [@cummings]. The ABS approach helps reach households without reliable phone access. We stratify our sample by geography, demographics, and previous voting patterns to ensure representation across key subgroups - meaning we divide the population into these subgroups and sample from each independently.

For non-probability sampling (40% of budget, i.e. $40k), where respondents' selection probabilities are unknown and not everyone has a chance of being selected, we recruit through multiple online panel vendors and use targeted social media advertising to reach traditionally underrepresented groups. While this approach introduces potential selection bias because participants self-select into the sample, it helps reach younger voters who are less responsive to traditional survey methods [@nonprobabilitysampling]. We implement quota sampling within these non-probability samples to match key demographic targets - for example, stopping collection from certain demographic groups once their quota is filled.

Respondent recruitment uses multiple contact methods - mail, email, text, and phone - with attempts made at different times and days to maximize response rates. We offer a $10 gift card incentive for completed surveys and provide both English and Spanish language options. This mixed-mode contact strategy helps reduce non-response bias by providing multiple ways to participate.

Data validation is crucial for maintaining quality. We cross-reference responses with voter files where available - meaning we check if respondents' self-reported registration status matches official records. We screen for duplicate responses using IP addresses and phone numbers, and implement attention checks within the survey (questions with known correct answers to ensure respondents are reading carefully). Speed checks identify rushed responses that might indicate low-quality data by flagging completions that fall below a minimum reasonable completion time, while consistency checks across related questions help identify potentially fraudulent responses by looking for logical contradictions in answers.

Our weighting approach uses post-stratification to known population benchmarks - this means we adjust the weight given to each response so that our sample matches known population characteristics. For example, if our sample has 30% college graduates but the population has 40%, we would give more weight to responses from college graduates. We include demographics (age, race, education, gender), geographic location, past voting behavior, and party registration in our weighting scheme. We produce daily estimates using a 7-day rolling average, which helps smooth out daily fluctuations while remaining responsive to real changes in voter preferences [@weighting2].

Finally, how ethics are handled is a crucial part of any survey methodology. In this idealised methodology, consent will be asked for at the beginning of the survey, full disclosure of how information is used will be given beforehand and no self-identifiable information will be recorded (so no names, phone numbers, etc.). The $10 incentive is enough to hopefully make it wortile for participants' time, but also not ideally not significant enough of an incentive to make individuals suppress otherwise deal-breaking concerns with the survey purely for the sake of the incentive.

The survey instrument itself focuses on six key areas: screening questions to identify likely voters, voting intentions (including direct questions about Trump vs. Harris preferences), political preferences, demographics, issue priorities, and media consumption patterns. We've implemented this survey design in Google Forms, which can be found here: https://forms.gle/pk7vDiMHwEGLMK849

This methodology balances statistical rigor with practical constraints, while acknowledging and attempting to address the key challenges in modern political polling: declining response rates, coverage bias, and the increasing difficulty of reaching a representative sample of likely voters.

## Idealised survey
The survey, made using Google Forms, is linked here: https://forms.gle/pk7vDiMHwEGLMK849
Note that the questions are identical for both the phone and online surveys bar q6.
A copy of the survey that is identical to the one implemented in the Google Forms above is presented below:
Thank you for participating in this survey about the 2024 U.S. Presidential Election. This survey is part of a research project at the University of Toronto studying voting intentions and political attitudes.

Estimated completion time: 8-10 minutes

Your responses will be kept confidential and used only for research purposes. Email information, and any other information that may personally identify you, is not gathered. You may skip any questions you prefer not to answer, though complete responses are most helpful for our research.

For questions or concerns about this survey, please contact: andrew.goh@mail.utoronto.ca

SCREENING SECTION:
Q1. Are you 18 years of age or older?

Yes
No [END SURVEY]

Q2. Are you a U.S. citizen?

Yes
No [END SURVEY]

Q3. Are you registered to vote at your current address?

Yes
No
Not sure
[If No or Not sure: Do you plan to register before the November 2024 election?]

VOTING INTENTION:
Q4. How likely are you to vote in the 2024 presidential election?

Definitely will vote
Probably will vote
Might or might not vote
Probably will not vote
Definitely will not vote

Q5. If the 2024 presidential election were held today, and the candidates were Kamala Harris (Democrat) and Donald Trump (Republican), who would you vote for?

Kamala Harris
Donald Trump
Another candidate (please specify)
Would not vote
Not sure

ATTENTION CHECK:
Q6. To ensure you're reading carefully, please select "Somewhat disagree" for this question: "I enjoy following political news."

Strongly agree
Somewhat agree
Somewhat disagree
Strongly disagree
No opinion

POLITICAL PREFERENCES:
Q7. Generally speaking, you consider yourself a:

Democrat
Republican
Independent
Something else (please specify)

Q8. How would you rate the current state of the U.S. economy?

Very poor
Poor
Fair
Good
Excellent

ISSUE PRIORITIES:
Q9. Which ONE of the following issues is most important to you when deciding how to vote?

Economy and jobs
Immigration
Healthcare
Climate change
Crime and public safety
Education
National security
Abortion rights
Gun policy
Something else (please specify)

For each of the following issues, please indicate whether you think Kamala Harris or Donald Trump would do a better job handling it:

Q10. Economy and jobs:

Kamala Harris would do better
Donald Trump would do better
No difference
Not sure

Q11. Human rights and freedom of speech:
[Same options]
Q12. Abortion:
[Same options]
Q13. Healthcare:
[Same options]
Q14. Immigration:
[Same options]
Q15. National security:
[Same options]

MEDIA CONSUMPTION:
Q16. Where do you most often get your news about politics? (Select all that apply)

Network TV news (ABC, CBS, NBC)
Cable TV news (CNN, Fox News, MSNBC)
Local TV news
Radio
Print newspapers
News websites
Social media
Friends and family
Other (please specify)

Q17. How many hours per day do you typically spend following news about politics?

Less than 1 hour
1-2 hours
2-4 hours
More than 4 hours

DEMOGRAPHICS:
Q18. What is your age?

18-24
25-34
35-44
45-54
55-64
65 or older

Q19. What is your gender?

Male
Female
Non-binary/Other
Prefer not to say

Q20. What is your race/ethnicity? (Select all that apply)

White
Black or African American
Hispanic or Latino
Asian
Native American
Other (please specify)
Prefer not to say

Q21. What is the highest level of education you have completed?

Less than high school
High school graduate
Some college
Associate's degree
Bachelor's degree
Graduate degree
Prefer not to say

Q22. What was your total household income before taxes in 2023?

Under $25,000
$25,000-$49,999
$50,000-$74,999
$75,000-$99,999
$100,000-$149,999
$150,000 or more
Prefer not to say

CONSISTENCY CHECK:
Q23. Looking ahead to November 2024, if Kamala Harris is the Democratic nominee and Donald Trump is the Republican nominee, how do you think you will vote?

Kamala Harris
Donald Trump
Another candidate (please specify)
Would not vote
Not sure

[END OF SURVEY]
Thank you for completing this survey about the 2024 U.S. Presidential Election. Your responses will help us better understand voter preferences and political attitudes across the country.
If you have any questions about this research or would like to be informed about the results, please contact andrew.goh@mail.utoronto.ca.
Your time and participation is greatly appreciated.

# Additional data details

# Model details {#sec-model-details}


## Diagnostics
```{r}
#| label: fig-pen
#| fig-cap: ROC Curve for Harris Win Prediction, Pennsylvania
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Pennsylvania)

```

```{r}
#| label: fig-geo
#| fig-cap: ROC Curve for Harris Win Prediction, Georgia
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Georgia)

```

```{r}
#| label: fig-nor
#| fig-cap: ROC Curve for Harris Win Prediction, North Carolina
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_North_Carolina)

```

```{r}
#| label: fig-mic
#| fig-cap: ROC Curve for Harris Win Prediction, Michigan
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Michigan)

```

```{r}
#| label: fig-ari
#| fig-cap: ROC Curve for Harris Win Prediction, Arizona
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Arizona)

```

```{r}
#| label: fig-wis
#| fig-cap: ROC Curve for Harris Win Prediction, Wisconsin
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Wisconsin)

```

```{r}
#| label: fig-nev
#| fig-cap: ROC Curve for Harris Win Prediction, Nevada
#| echo: false
#| message: false
#| warning: false

plot(roc_obj_Nevada)

```

```{r}
#| label: fig-log
#| fig-cap: ROC Curve for Popular Vote Prediction
#| echo: false
#| message: false
#| warning: false

plot(roc_obj)

```

```{r}
#| label: fig-logadj
#| fig-cap: ROC Curve for Popular Vote Prediction, Adjusted
#| echo: false
#| message: false
#| warning: false

plot(roc_obj)

```
\FloatBarrier

## Coefficients

```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_Pennsylvania <- tidy(logistic_Pennsylvania)

# Display the table using knitr::kable
kable(tidy_Pennsylvania, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for Pennsylvania")

```
```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_Georgia <- tidy(logistic_Georgia)

# Display the table using knitr::kable
kable(tidy_Georgia, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for Georgia")

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_Michigan <- tidy(logistic_Michigan)

# Display the table using knitr::kable
kable(tidy_Michigan, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for Michigan")

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_North_Carolina <- tidy(logistic_North_Carolina)

# Display the table using knitr::kable
kable(tidy_North_Carolina, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for North_Carolina")

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_Arizona <- tidy(logistic_Arizona)

# Display the table using knitr::kable
kable(tidy_Arizona, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for Arizona")

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_Wisconsin <- tidy(logistic_Wisconsin)

# Display the table using knitr::kable
kable(tidy_Wisconsin, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for Wisconsin")

```

```{r}
#| echo: false
#| message: false
#| warning: false
# Tidy the logistic regression model
tidy_Nevada <- tidy(logistic_Nevada)

# Display the table using knitr::kable
kable(tidy_Nevada, digits = 2, 
      col.names = c("Term", "Estimate", "Std. Error", "Statistic", "p-value"),
      caption = "Logistic Regression Coefficients for Nevada")

```

\newpage


# References


